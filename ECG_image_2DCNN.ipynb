{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rameshavinash94/Cardiovascular-Detection-using-ECG-images/blob/main/Merging_Scaled_1D_%26_Trying_Different_CLassification_ML_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIqvxE_BKjV6"
   },
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C82bu1OsKnfC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from natsort import natsorted\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXx7SoC7qyRv"
   },
   "source": [
    "### **WORKING ON COMBING MULTIPLE LEAD FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sI0sFzAqPP8g"
   },
   "outputs": [],
   "source": [
    "#creating list to store file_names\n",
    "NORMAL_=[]\n",
    "MI_=[]\n",
    "PMI_=[]\n",
    "HB_=[]\n",
    "\n",
    "normal = 'Normal Person ECG Images (284x12=3408)'\n",
    "abnormal = 'ECG Images of Patient that have abnormal heartbeat (233x12=2796)'\n",
    "MI = 'ECG Images of Myocardial Infarction Patients (240x12=2880)'\n",
    "MI_history = 'ECG Images of Patient that have History of MI (172x12=2064)'\n",
    "\n",
    "Types_ECG = {'normal':normal,'Abnormal_hear_beat':abnormal,'MI':MI,'History_MI':MI_history}\n",
    "\n",
    "for types,folder in Types_ECG.items():\n",
    "  for files in os.listdir(folder):\n",
    "    if types=='normal':\n",
    "      NORMAL_.append(files)\n",
    "    elif types=='Abnormal_hear_beat':\n",
    "      HB_.append(files)\n",
    "    elif types=='MI':\n",
    "      MI_.append(files)\n",
    "    elif types=='History_MI':\n",
    "      PMI_.append(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJL9qAFSUOsN",
    "outputId": "9a4817f5-ae50-4aaf-96fe-8cc945289ad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Normal(1).jpg',\n",
       " 'Normal(2).jpg',\n",
       " 'Normal(3).jpg',\n",
       " 'Normal(4).jpg',\n",
       " 'Normal(5).jpg',\n",
       " 'Normal(6).jpg',\n",
       " 'Normal(7).jpg',\n",
       " 'Normal(8).jpg',\n",
       " 'Normal(9).jpg',\n",
       " 'Normal(10).jpg',\n",
       " 'Normal(11).jpg',\n",
       " 'Normal(12).jpg',\n",
       " 'Normal(13).jpg',\n",
       " 'Normal(14).jpg',\n",
       " 'Normal(15).jpg',\n",
       " 'Normal(16).jpg',\n",
       " 'Normal(17).jpg',\n",
       " 'Normal(18).jpg',\n",
       " 'Normal(19).jpg',\n",
       " 'Normal(20).jpg',\n",
       " 'Normal(21).jpg',\n",
       " 'Normal(22).jpg',\n",
       " 'Normal(23).jpg',\n",
       " 'Normal(24).jpg',\n",
       " 'Normal(25).jpg',\n",
       " 'Normal(26).jpg',\n",
       " 'Normal(27).jpg',\n",
       " 'Normal(28).jpg',\n",
       " 'Normal(29).jpg',\n",
       " 'Normal(30).jpg',\n",
       " 'Normal(31).jpg',\n",
       " 'Normal(32).jpg',\n",
       " 'Normal(33).jpg',\n",
       " 'Normal(34).jpg',\n",
       " 'Normal(35).jpg',\n",
       " 'Normal(36).jpg',\n",
       " 'Normal(37).jpg',\n",
       " 'Normal(38).jpg',\n",
       " 'Normal(39).jpg',\n",
       " 'Normal(40).jpg',\n",
       " 'Normal(41).jpg',\n",
       " 'Normal(42).jpg',\n",
       " 'Normal(43).jpg',\n",
       " 'Normal(44).jpg',\n",
       " 'Normal(45).jpg',\n",
       " 'Normal(46).jpg',\n",
       " 'Normal(47).jpg',\n",
       " 'Normal(48).jpg',\n",
       " 'Normal(49).jpg',\n",
       " 'Normal(50).jpg',\n",
       " 'Normal(51).jpg',\n",
       " 'Normal(52).jpg',\n",
       " 'Normal(53).jpg',\n",
       " 'Normal(54).jpg',\n",
       " 'Normal(55).jpg',\n",
       " 'Normal(56).jpg',\n",
       " 'Normal(57).jpg',\n",
       " 'Normal(58).jpg',\n",
       " 'Normal(59).jpg',\n",
       " 'Normal(60).jpg',\n",
       " 'Normal(61).jpg',\n",
       " 'Normal(62).jpg',\n",
       " 'Normal(63).jpg',\n",
       " 'Normal(64).jpg',\n",
       " 'Normal(65).jpg',\n",
       " 'Normal(66).jpg',\n",
       " 'Normal(67).jpg',\n",
       " 'Normal(68).jpg',\n",
       " 'Normal(69).jpg',\n",
       " 'Normal(70).jpg',\n",
       " 'Normal(71).jpg',\n",
       " 'Normal(72).jpg',\n",
       " 'Normal(73).jpg',\n",
       " 'Normal(74).jpg',\n",
       " 'Normal(75).jpg',\n",
       " 'Normal(76).jpg',\n",
       " 'Normal(77).jpg',\n",
       " 'Normal(78).jpg',\n",
       " 'Normal(79).jpg',\n",
       " 'Normal(80).jpg',\n",
       " 'Normal(81).jpg',\n",
       " 'Normal(82).jpg',\n",
       " 'Normal(83).jpg',\n",
       " 'Normal(84).jpg',\n",
       " 'Normal(85).jpg',\n",
       " 'Normal(86).jpg',\n",
       " 'Normal(87).jpg',\n",
       " 'Normal(88).jpg',\n",
       " 'Normal(89).jpg',\n",
       " 'Normal(90).jpg',\n",
       " 'Normal(91).jpg',\n",
       " 'Normal(92).jpg',\n",
       " 'Normal(93).jpg',\n",
       " 'Normal(94).jpg',\n",
       " 'Normal(95).jpg',\n",
       " 'Normal(96).jpg',\n",
       " 'Normal(97).jpg',\n",
       " 'Normal(98).jpg',\n",
       " 'Normal(99).jpg',\n",
       " 'Normal(100).jpg',\n",
       " 'Normal(101).jpg',\n",
       " 'Normal(102).jpg',\n",
       " 'Normal(103).jpg',\n",
       " 'Normal(104).jpg',\n",
       " 'Normal(105).jpg',\n",
       " 'Normal(106).jpg',\n",
       " 'Normal(107).jpg',\n",
       " 'Normal(108).jpg',\n",
       " 'Normal(109).jpg',\n",
       " 'Normal(110).jpg',\n",
       " 'Normal(111).jpg',\n",
       " 'Normal(112).jpg',\n",
       " 'Normal(113).jpg',\n",
       " 'Normal(114).jpg',\n",
       " 'Normal(115).jpg',\n",
       " 'Normal(116).jpg',\n",
       " 'Normal(117).jpg',\n",
       " 'Normal(118).jpg',\n",
       " 'Normal(119).jpg',\n",
       " 'Normal(120).jpg',\n",
       " 'Normal(121).jpg',\n",
       " 'Normal(122).jpg',\n",
       " 'Normal(123).jpg',\n",
       " 'Normal(124).jpg',\n",
       " 'Normal(125).jpg',\n",
       " 'Normal(126).jpg',\n",
       " 'Normal(127).jpg',\n",
       " 'Normal(128).jpg',\n",
       " 'Normal(129).jpg',\n",
       " 'Normal(130).jpg',\n",
       " 'Normal(131).jpg',\n",
       " 'Normal(132).jpg',\n",
       " 'Normal(133).jpg',\n",
       " 'Normal(134).jpg',\n",
       " 'Normal(135).jpg',\n",
       " 'Normal(136).jpg',\n",
       " 'Normal(137).jpg',\n",
       " 'Normal(138).jpg',\n",
       " 'Normal(139).jpg',\n",
       " 'Normal(140).jpg',\n",
       " 'Normal(141).jpg',\n",
       " 'Normal(142).jpg',\n",
       " 'Normal(143).jpg',\n",
       " 'Normal(144).jpg',\n",
       " 'Normal(145).jpg',\n",
       " 'Normal(146).jpg',\n",
       " 'Normal(147).jpg',\n",
       " 'Normal(148).jpg',\n",
       " 'Normal(149).jpg',\n",
       " 'Normal(150).jpg',\n",
       " 'Normal(151).jpg',\n",
       " 'Normal(152).jpg',\n",
       " 'Normal(153).jpg',\n",
       " 'Normal(154).jpg',\n",
       " 'Normal(155).jpg',\n",
       " 'Normal(156).jpg',\n",
       " 'Normal(157).jpg',\n",
       " 'Normal(158).jpg',\n",
       " 'Normal(159).jpg',\n",
       " 'Normal(160).jpg',\n",
       " 'Normal(161).jpg',\n",
       " 'Normal(162).jpg',\n",
       " 'Normal(163).jpg',\n",
       " 'Normal(164).jpg',\n",
       " 'Normal(165).jpg',\n",
       " 'Normal(166).jpg',\n",
       " 'Normal(167).jpg',\n",
       " 'Normal(168).jpg',\n",
       " 'Normal(169).jpg',\n",
       " 'Normal(170).jpg',\n",
       " 'Normal(171).jpg',\n",
       " 'Normal(172).jpg',\n",
       " 'Normal(173).jpg',\n",
       " 'Normal(174).jpg',\n",
       " 'Normal(175).jpg',\n",
       " 'Normal(176).jpg',\n",
       " 'Normal(177).jpg',\n",
       " 'Normal(178).jpg',\n",
       " 'Normal(179).jpg',\n",
       " 'Normal(180).jpg',\n",
       " 'Normal(181).jpg',\n",
       " 'Normal(182).jpg',\n",
       " 'Normal(183).jpg',\n",
       " 'Normal(184).jpg',\n",
       " 'Normal(185).jpg',\n",
       " 'Normal(186).jpg',\n",
       " 'Normal(187).jpg',\n",
       " 'Normal(188).jpg',\n",
       " 'Normal(189).jpg',\n",
       " 'Normal(190).jpg',\n",
       " 'Normal(191).jpg',\n",
       " 'Normal(192).jpg',\n",
       " 'Normal(193).jpg',\n",
       " 'Normal(194).jpg',\n",
       " 'Normal(195).jpg',\n",
       " 'Normal(196).jpg',\n",
       " 'Normal(197).jpg',\n",
       " 'Normal(198).jpg',\n",
       " 'Normal(199).jpg',\n",
       " 'Normal(200).jpg',\n",
       " 'Normal(201).jpg',\n",
       " 'Normal(202).jpg',\n",
       " 'Normal(203).jpg',\n",
       " 'Normal(204).jpg',\n",
       " 'Normal(205).jpg',\n",
       " 'Normal(206).jpg',\n",
       " 'Normal(207).jpg',\n",
       " 'Normal(208).jpg',\n",
       " 'Normal(209).jpg',\n",
       " 'Normal(210).jpg',\n",
       " 'Normal(211).jpg',\n",
       " 'Normal(212).jpg',\n",
       " 'Normal(213).jpg',\n",
       " 'Normal(214).jpg',\n",
       " 'Normal(215).jpg',\n",
       " 'Normal(216).jpg',\n",
       " 'Normal(217).jpg',\n",
       " 'Normal(218).jpg',\n",
       " 'Normal(219).jpg',\n",
       " 'Normal(220).jpg',\n",
       " 'Normal(221).jpg',\n",
       " 'Normal(222).jpg',\n",
       " 'Normal(223).jpg',\n",
       " 'Normal(224).jpg',\n",
       " 'Normal(225).jpg',\n",
       " 'Normal(226).jpg',\n",
       " 'Normal(227).jpg',\n",
       " 'Normal(228).jpg',\n",
       " 'Normal(229).jpg',\n",
       " 'Normal(230).jpg',\n",
       " 'Normal(231).jpg',\n",
       " 'Normal(232).jpg',\n",
       " 'Normal(233).jpg',\n",
       " 'Normal(234).jpg',\n",
       " 'Normal(235).jpg',\n",
       " 'Normal(236).jpg',\n",
       " 'Normal(237).jpg',\n",
       " 'Normal(238).jpg',\n",
       " 'Normal(239).jpg',\n",
       " 'Normal(240).jpg',\n",
       " 'Normal(241).jpg',\n",
       " 'Normal(242).jpg',\n",
       " 'Normal(243).jpg',\n",
       " 'Normal(244).jpg',\n",
       " 'Normal(245).jpg',\n",
       " 'Normal(246).jpg',\n",
       " 'Normal(247).jpg',\n",
       " 'Normal(248).jpg',\n",
       " 'Normal(249).jpg',\n",
       " 'Normal(250).jpg',\n",
       " 'Normal(251).jpg',\n",
       " 'Normal(252).jpg',\n",
       " 'Normal(253).jpg',\n",
       " 'Normal(254).jpg',\n",
       " 'Normal(255).jpg',\n",
       " 'Normal(256).jpg',\n",
       " 'Normal(257).jpg',\n",
       " 'Normal(258).jpg',\n",
       " 'Normal(259).jpg',\n",
       " 'Normal(260).jpg',\n",
       " 'Normal(261).jpg',\n",
       " 'Normal(262).jpg',\n",
       " 'Normal(263).jpg',\n",
       " 'Normal(264).jpg',\n",
       " 'Normal(265).jpg',\n",
       " 'Normal(266).jpg',\n",
       " 'Normal(267).jpg',\n",
       " 'Normal(268).jpg',\n",
       " 'Normal(269).jpg',\n",
       " 'Normal(270).jpg',\n",
       " 'Normal(271).jpg',\n",
       " 'Normal(272).jpg',\n",
       " 'Normal(273).jpg',\n",
       " 'Normal(274).jpg',\n",
       " 'Normal(275).jpg',\n",
       " 'Normal(276).jpg',\n",
       " 'Normal(277).jpg',\n",
       " 'Normal(278).jpg',\n",
       " 'Normal(279).jpg',\n",
       " 'Normal(280).jpg',\n",
       " 'Normal(281).jpg',\n",
       " 'Normal(282).jpg',\n",
       " 'Normal(283).jpg',\n",
       " 'Normal(284).jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMAL_ = natsorted(NORMAL_)\n",
    "NORMAL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRbNR2HEUkvU",
    "outputId": "0cd461c0-5fe2-463a-cbf5-6325662d9011"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MI(1).jpg',\n",
       " 'MI(2).jpg',\n",
       " 'MI(3).jpg',\n",
       " 'MI(4).jpg',\n",
       " 'MI(5).jpg',\n",
       " 'MI(6).jpg',\n",
       " 'MI(7).jpg',\n",
       " 'MI(8).jpg',\n",
       " 'MI(9).jpg',\n",
       " 'MI(10).jpg',\n",
       " 'MI(11).jpg',\n",
       " 'MI(12).jpg',\n",
       " 'MI(13).jpg',\n",
       " 'MI(14).jpg',\n",
       " 'MI(15).jpg',\n",
       " 'MI(16).jpg',\n",
       " 'MI(17).jpg',\n",
       " 'MI(18).jpg',\n",
       " 'MI(19).jpg',\n",
       " 'MI(20).jpg',\n",
       " 'MI(21).jpg',\n",
       " 'MI(22).jpg',\n",
       " 'MI(23).jpg',\n",
       " 'MI(24).jpg',\n",
       " 'MI(25).jpg',\n",
       " 'MI(26).jpg',\n",
       " 'MI(27).jpg',\n",
       " 'MI(28).jpg',\n",
       " 'MI(29).jpg',\n",
       " 'MI(30).jpg',\n",
       " 'MI(31).jpg',\n",
       " 'MI(32).jpg',\n",
       " 'MI(33).jpg',\n",
       " 'MI(34).jpg',\n",
       " 'MI(35).jpg',\n",
       " 'MI(36).jpg',\n",
       " 'MI(37).jpg',\n",
       " 'MI(38).jpg',\n",
       " 'MI(39).jpg',\n",
       " 'MI(40).jpg',\n",
       " 'MI(41).jpg',\n",
       " 'MI(42).jpg',\n",
       " 'MI(43).jpg',\n",
       " 'MI(44).jpg',\n",
       " 'MI(45).jpg',\n",
       " 'MI(46).jpg',\n",
       " 'MI(47).jpg',\n",
       " 'MI(48).jpg',\n",
       " 'MI(49).jpg',\n",
       " 'MI(50).jpg',\n",
       " 'MI(51).jpg',\n",
       " 'MI(52).jpg',\n",
       " 'MI(53).jpg',\n",
       " 'MI(54).jpg',\n",
       " 'MI(55).jpg',\n",
       " 'MI(56).jpg',\n",
       " 'MI(57).jpg',\n",
       " 'MI(58).jpg',\n",
       " 'MI(59).jpg',\n",
       " 'MI(60).jpg',\n",
       " 'MI(61).jpg',\n",
       " 'MI(62).jpg',\n",
       " 'MI(63).jpg',\n",
       " 'MI(64).jpg',\n",
       " 'MI(65).jpg',\n",
       " 'MI(66).jpg',\n",
       " 'MI(67).jpg',\n",
       " 'MI(68).jpg',\n",
       " 'MI(69).jpg',\n",
       " 'MI(70).jpg',\n",
       " 'MI(71).jpg',\n",
       " 'MI(72).jpg',\n",
       " 'MI(73).jpg',\n",
       " 'MI(74).jpg',\n",
       " 'MI(75).jpg',\n",
       " 'MI(76).jpg',\n",
       " 'MI(77).jpg',\n",
       " 'MI(78).jpg',\n",
       " 'MI(79).jpg',\n",
       " 'MI(80).jpg',\n",
       " 'MI(81).jpg',\n",
       " 'MI(82).jpg',\n",
       " 'MI(83).jpg',\n",
       " 'MI(84).jpg',\n",
       " 'MI(85).jpg',\n",
       " 'MI(86).jpg',\n",
       " 'MI(87).jpg',\n",
       " 'MI(88).jpg',\n",
       " 'MI(89).jpg',\n",
       " 'MI(90).jpg',\n",
       " 'MI(91).jpg',\n",
       " 'MI(92).jpg',\n",
       " 'MI(93).jpg',\n",
       " 'MI(94).jpg',\n",
       " 'MI(95).jpg',\n",
       " 'MI(96).jpg',\n",
       " 'MI(97).jpg',\n",
       " 'MI(98).jpg',\n",
       " 'MI(99).jpg',\n",
       " 'MI(100).jpg',\n",
       " 'MI(101).jpg',\n",
       " 'MI(102).jpg',\n",
       " 'MI(103).jpg',\n",
       " 'MI(104).jpg',\n",
       " 'MI(105).jpg',\n",
       " 'MI(106).jpg',\n",
       " 'MI(107).jpg',\n",
       " 'MI(108).jpg',\n",
       " 'MI(109).jpg',\n",
       " 'MI(110).jpg',\n",
       " 'MI(111).jpg',\n",
       " 'MI(112).jpg',\n",
       " 'MI(113).jpg',\n",
       " 'MI(114).jpg',\n",
       " 'MI(115).jpg',\n",
       " 'MI(116).jpg',\n",
       " 'MI(117).jpg',\n",
       " 'MI(118).jpg',\n",
       " 'MI(119).jpg',\n",
       " 'MI(120).jpg',\n",
       " 'MI(121).jpg',\n",
       " 'MI(122).jpg',\n",
       " 'MI(123).jpg',\n",
       " 'MI(124).jpg',\n",
       " 'MI(125).jpg',\n",
       " 'MI(126).jpg',\n",
       " 'MI(127).jpg',\n",
       " 'MI(128).jpg',\n",
       " 'MI(129).jpg',\n",
       " 'MI(130).jpg',\n",
       " 'MI(131).jpg',\n",
       " 'MI(132).jpg',\n",
       " 'MI(133).jpg',\n",
       " 'MI(134).jpg',\n",
       " 'MI(135).jpg',\n",
       " 'MI(136).jpg',\n",
       " 'MI(137).jpg',\n",
       " 'MI(138).jpg',\n",
       " 'MI(139).jpg',\n",
       " 'MI(140).jpg',\n",
       " 'MI(141).jpg',\n",
       " 'MI(142).jpg',\n",
       " 'MI(143).jpg',\n",
       " 'MI(144).jpg',\n",
       " 'MI(145).jpg',\n",
       " 'MI(146).jpg',\n",
       " 'MI(147).jpg',\n",
       " 'MI(148).jpg',\n",
       " 'MI(149).jpg',\n",
       " 'MI(150).jpg',\n",
       " 'MI(151).jpg',\n",
       " 'MI(152).jpg',\n",
       " 'MI(153).jpg',\n",
       " 'MI(154).jpg',\n",
       " 'MI(155).jpg',\n",
       " 'MI(156).jpg',\n",
       " 'MI(157).jpg',\n",
       " 'MI(158).jpg',\n",
       " 'MI(159).jpg',\n",
       " 'MI(160).jpg',\n",
       " 'MI(161).jpg',\n",
       " 'MI(162).jpg',\n",
       " 'MI(163).jpg',\n",
       " 'MI(164).jpg',\n",
       " 'MI(165).jpg',\n",
       " 'MI(166).jpg',\n",
       " 'MI(167).jpg',\n",
       " 'MI(168).jpg',\n",
       " 'MI(169).jpg',\n",
       " 'MI(170).jpg',\n",
       " 'MI(171).jpg',\n",
       " 'MI(172).jpg',\n",
       " 'MI(173).jpg',\n",
       " 'MI(174).jpg',\n",
       " 'MI(175).jpg',\n",
       " 'MI(176).jpg',\n",
       " 'MI(177).jpg',\n",
       " 'MI(178).jpg',\n",
       " 'MI(179).jpg',\n",
       " 'MI(180).jpg',\n",
       " 'MI(181).jpg',\n",
       " 'MI(182).jpg',\n",
       " 'MI(183).jpg',\n",
       " 'MI(184).jpg',\n",
       " 'MI(185).jpg',\n",
       " 'MI(186).jpg',\n",
       " 'MI(187).jpg',\n",
       " 'MI(188).jpg',\n",
       " 'MI(189).jpg',\n",
       " 'MI(190).jpg',\n",
       " 'MI(191).jpg',\n",
       " 'MI(192).jpg',\n",
       " 'MI(193).jpg',\n",
       " 'MI(194).jpg',\n",
       " 'MI(195).jpg',\n",
       " 'MI(196).jpg',\n",
       " 'MI(197).jpg',\n",
       " 'MI(198).jpg',\n",
       " 'MI(199).jpg',\n",
       " 'MI(200).jpg',\n",
       " 'MI(201).jpg',\n",
       " 'MI(202).jpg',\n",
       " 'MI(203).jpg',\n",
       " 'MI(204).jpg',\n",
       " 'MI(205).jpg',\n",
       " 'MI(206).jpg',\n",
       " 'MI(207).jpg',\n",
       " 'MI(208).jpg',\n",
       " 'MI(209).jpg',\n",
       " 'MI(210).jpg',\n",
       " 'MI(211).jpg',\n",
       " 'MI(212).jpg',\n",
       " 'MI(213).jpg',\n",
       " 'MI(214).jpg',\n",
       " 'MI(216).jpg',\n",
       " 'MI(217).jpg',\n",
       " 'MI(218).jpg',\n",
       " 'MI(219).jpg',\n",
       " 'MI(220).jpg',\n",
       " 'MI(221).jpg',\n",
       " 'MI(222).jpg',\n",
       " 'MI(223).jpg',\n",
       " 'MI(224).jpg',\n",
       " 'MI(225).jpg',\n",
       " 'MI(226).jpg',\n",
       " 'MI(227).jpg',\n",
       " 'MI(228).jpg',\n",
       " 'MI(229).jpg',\n",
       " 'MI(230).jpg',\n",
       " 'MI(231).jpg',\n",
       " 'MI(232).jpg',\n",
       " 'MI(233).jpg',\n",
       " 'MI(234).jpg',\n",
       " 'MI(235).jpg',\n",
       " 'MI(236).jpg',\n",
       " 'MI(237).jpg',\n",
       " 'MI(238).jpg',\n",
       " 'MI(239).jpg',\n",
       " 'MI(240).jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_ = natsorted(MI_)\n",
    "MI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4LMfTU3UuCM",
    "outputId": "0c127e8e-62b7-4015-e3ee-a34860e4f5ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PMI(1).jpg',\n",
       " 'PMI(2).jpg',\n",
       " 'PMI(3).jpg',\n",
       " 'PMI(4).jpg',\n",
       " 'PMI(5).jpg',\n",
       " 'PMI(6).jpg',\n",
       " 'PMI(7).jpg',\n",
       " 'PMI(8).jpg',\n",
       " 'PMI(9).jpg',\n",
       " 'PMI(10).jpg',\n",
       " 'PMI(11).jpg',\n",
       " 'PMI(12).jpg',\n",
       " 'PMI(13).jpg',\n",
       " 'PMI(14).jpg',\n",
       " 'PMI(15).jpg',\n",
       " 'PMI(16).jpg',\n",
       " 'PMI(17).jpg',\n",
       " 'PMI(18).jpg',\n",
       " 'PMI(19).jpg',\n",
       " 'PMI(20).jpg',\n",
       " 'PMI(21).jpg',\n",
       " 'PMI(22).jpg',\n",
       " 'PMI(23).jpg',\n",
       " 'PMI(24).jpg',\n",
       " 'PMI(25).jpg',\n",
       " 'PMI(26).jpg',\n",
       " 'PMI(27).jpg',\n",
       " 'PMI(28).jpg',\n",
       " 'PMI(29).jpg',\n",
       " 'PMI(30).jpg',\n",
       " 'PMI(31).jpg',\n",
       " 'PMI(32).jpg',\n",
       " 'PMI(33).jpg',\n",
       " 'PMI(34).jpg',\n",
       " 'PMI(35).jpg',\n",
       " 'PMI(36).jpg',\n",
       " 'PMI(37).jpg',\n",
       " 'PMI(38).jpg',\n",
       " 'PMI(39).jpg',\n",
       " 'PMI(40).jpg',\n",
       " 'PMI(41).jpg',\n",
       " 'PMI(42).jpg',\n",
       " 'PMI(43).jpg',\n",
       " 'PMI(44).jpg',\n",
       " 'PMI(45).jpg',\n",
       " 'PMI(46).jpg',\n",
       " 'PMI(47).jpg',\n",
       " 'PMI(48).jpg',\n",
       " 'PMI(49).jpg',\n",
       " 'PMI(50).jpg',\n",
       " 'PMI(51).jpg',\n",
       " 'PMI(52).jpg',\n",
       " 'PMI(53).jpg',\n",
       " 'PMI(54).jpg',\n",
       " 'PMI(55).jpg',\n",
       " 'PMI(56).jpg',\n",
       " 'PMI(57).jpg',\n",
       " 'PMI(58).jpg',\n",
       " 'PMI(59).jpg',\n",
       " 'PMI(60).jpg',\n",
       " 'PMI(61).jpg',\n",
       " 'PMI(62).jpg',\n",
       " 'PMI(63).jpg',\n",
       " 'PMI(64).jpg',\n",
       " 'PMI(65).jpg',\n",
       " 'PMI(66).jpg',\n",
       " 'PMI(67).jpg',\n",
       " 'PMI(68).jpg',\n",
       " 'PMI(69).jpg',\n",
       " 'PMI(70).jpg',\n",
       " 'PMI(71).jpg',\n",
       " 'PMI(72).jpg',\n",
       " 'PMI(73).jpg',\n",
       " 'PMI(74).jpg',\n",
       " 'PMI(75).jpg',\n",
       " 'PMI(76).jpg',\n",
       " 'PMI(77).jpg',\n",
       " 'PMI(78).jpg',\n",
       " 'PMI(79).jpg',\n",
       " 'PMI(80).jpg',\n",
       " 'PMI(81).jpg',\n",
       " 'PMI(82).jpg',\n",
       " 'PMI(83).jpg',\n",
       " 'PMI(84).jpg',\n",
       " 'PMI(85).jpg',\n",
       " 'PMI(86).jpg',\n",
       " 'PMI(87).jpg',\n",
       " 'PMI(88).jpg',\n",
       " 'PMI(89).jpg',\n",
       " 'PMI(90).jpg',\n",
       " 'PMI(91).jpg',\n",
       " 'PMI(92).jpg',\n",
       " 'PMI(93).jpg',\n",
       " 'PMI(94).jpg',\n",
       " 'PMI(95).jpg',\n",
       " 'PMI(96).jpg',\n",
       " 'PMI(97).jpg',\n",
       " 'PMI(98).jpg',\n",
       " 'PMI(99).jpg',\n",
       " 'PMI(100).jpg',\n",
       " 'PMI(101).jpg',\n",
       " 'PMI(102).jpg',\n",
       " 'PMI(103).jpg',\n",
       " 'PMI(104).jpg',\n",
       " 'PMI(105).jpg',\n",
       " 'PMI(106).jpg',\n",
       " 'PMI(107).jpg',\n",
       " 'PMI(108).jpg',\n",
       " 'PMI(109).jpg',\n",
       " 'PMI(110).jpg',\n",
       " 'PMI(111).jpg',\n",
       " 'PMI(112).jpg',\n",
       " 'PMI(113).jpg',\n",
       " 'PMI(114).jpg',\n",
       " 'PMI(115).jpg',\n",
       " 'PMI(116).jpg',\n",
       " 'PMI(117).jpg',\n",
       " 'PMI(118).jpg',\n",
       " 'PMI(119).jpg',\n",
       " 'PMI(120).jpg',\n",
       " 'PMI(121).jpg',\n",
       " 'PMI(122).jpg',\n",
       " 'PMI(123).jpg',\n",
       " 'PMI(124).jpg',\n",
       " 'PMI(125).jpg',\n",
       " 'PMI(126).jpg',\n",
       " 'PMI(127).jpg',\n",
       " 'PMI(128).jpg',\n",
       " 'PMI(129).jpg',\n",
       " 'PMI(130).jpg',\n",
       " 'PMI(131).jpg',\n",
       " 'PMI(132).jpg',\n",
       " 'PMI(133).jpg',\n",
       " 'PMI(134).jpg',\n",
       " 'PMI(135).jpg',\n",
       " 'PMI(136).jpg',\n",
       " 'PMI(137).jpg',\n",
       " 'PMI(138).jpg',\n",
       " 'PMI(139).jpg',\n",
       " 'PMI(140).jpg',\n",
       " 'PMI(141).jpg',\n",
       " 'PMI(142).jpg',\n",
       " 'PMI(143).jpg',\n",
       " 'PMI(144).jpg',\n",
       " 'PMI(145).jpg',\n",
       " 'PMI(146).jpg',\n",
       " 'PMI(147).jpg',\n",
       " 'PMI(148).jpg',\n",
       " 'PMI(149).jpg',\n",
       " 'PMI(150).jpg',\n",
       " 'PMI(151).jpg',\n",
       " 'PMI(152).jpg',\n",
       " 'PMI(153).jpg',\n",
       " 'PMI(154).jpg',\n",
       " 'PMI(155).jpg',\n",
       " 'PMI(156).jpg',\n",
       " 'PMI(157).jpg',\n",
       " 'PMI(158).jpg',\n",
       " 'PMI(159).jpg',\n",
       " 'PMI(160).jpg',\n",
       " 'PMI(161).jpg',\n",
       " 'PMI(162).jpg',\n",
       " 'PMI(163).jpg',\n",
       " 'PMI(164).jpg',\n",
       " 'PMI(165).jpg',\n",
       " 'PMI(166).jpg',\n",
       " 'PMI(167).jpg',\n",
       " 'PMI(168).jpg',\n",
       " 'PMI(169).jpg',\n",
       " 'PMI(170).jpg',\n",
       " 'PMI(171).jpg',\n",
       " 'PMI(172).jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMI_ = natsorted(PMI_)\n",
    "PMI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OinhdMKtU5mf",
    "outputId": "52dd393b-76c2-44a9-e64e-4d428380ed8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HB(1).jpg',\n",
       " 'HB(2).jpg',\n",
       " 'HB(3).jpg',\n",
       " 'HB(4).jpg',\n",
       " 'HB(5).jpg',\n",
       " 'HB(6).jpg',\n",
       " 'HB(7).jpg',\n",
       " 'HB(8).jpg',\n",
       " 'HB(9).jpg',\n",
       " 'HB(10).jpg',\n",
       " 'HB(11).jpg',\n",
       " 'HB(12).jpg',\n",
       " 'HB(13).jpg',\n",
       " 'HB(14).jpg',\n",
       " 'HB(15).jpg',\n",
       " 'HB(16).jpg',\n",
       " 'HB(17).jpg',\n",
       " 'HB(18).jpg',\n",
       " 'HB(19).jpg',\n",
       " 'HB(20).jpg',\n",
       " 'HB(21).jpg',\n",
       " 'HB(22).jpg',\n",
       " 'HB(23).jpg',\n",
       " 'HB(24).jpg',\n",
       " 'HB(25).jpg',\n",
       " 'HB(26).jpg',\n",
       " 'HB(27).jpg',\n",
       " 'HB(28).jpg',\n",
       " 'HB(29).jpg',\n",
       " 'HB(30).jpg',\n",
       " 'HB(31).jpg',\n",
       " 'HB(32).jpg',\n",
       " 'HB(33).jpg',\n",
       " 'HB(34).jpg',\n",
       " 'HB(35).jpg',\n",
       " 'HB(36).jpg',\n",
       " 'HB(37).jpg',\n",
       " 'HB(38).jpg',\n",
       " 'HB(39).jpg',\n",
       " 'HB(40).jpg',\n",
       " 'HB(41).jpg',\n",
       " 'HB(42).jpg',\n",
       " 'HB(43).jpg',\n",
       " 'HB(44).jpg',\n",
       " 'HB(45).jpg',\n",
       " 'HB(46).jpg',\n",
       " 'HB(47).jpg',\n",
       " 'HB(48).jpg',\n",
       " 'HB(49).jpg',\n",
       " 'HB(50).jpg',\n",
       " 'HB(51).jpg',\n",
       " 'HB(52).jpg',\n",
       " 'HB(53).jpg',\n",
       " 'HB(54).jpg',\n",
       " 'HB(55).jpg',\n",
       " 'HB(56).jpg',\n",
       " 'HB(57).jpg',\n",
       " 'HB(58).jpg',\n",
       " 'HB(59).jpg',\n",
       " 'HB(60).jpg',\n",
       " 'HB(61).jpg',\n",
       " 'HB(62).jpg',\n",
       " 'HB(63).jpg',\n",
       " 'HB(64).jpg',\n",
       " 'HB(65).jpg',\n",
       " 'HB(66).jpg',\n",
       " 'HB(67).jpg',\n",
       " 'HB(68).jpg',\n",
       " 'HB(69).jpg',\n",
       " 'HB(70).jpg',\n",
       " 'HB(71).jpg',\n",
       " 'HB(72).jpg',\n",
       " 'HB(73).jpg',\n",
       " 'HB(74).jpg',\n",
       " 'HB(75).jpg',\n",
       " 'HB(76).jpg',\n",
       " 'HB(77).jpg',\n",
       " 'HB(78).jpg',\n",
       " 'HB(79).jpg',\n",
       " 'HB(80).jpg',\n",
       " 'HB(81).jpg',\n",
       " 'HB(82).jpg',\n",
       " 'HB(83).jpg',\n",
       " 'HB(84).jpg',\n",
       " 'HB(85).jpg',\n",
       " 'HB(86).jpg',\n",
       " 'HB(87).jpg',\n",
       " 'HB(88).jpg',\n",
       " 'HB(89).jpg',\n",
       " 'HB(90).jpg',\n",
       " 'HB(91).jpg',\n",
       " 'HB(92).jpg',\n",
       " 'HB(93).jpg',\n",
       " 'HB(94).jpg',\n",
       " 'HB(95).jpg',\n",
       " 'HB(96).jpg',\n",
       " 'HB(97).jpg',\n",
       " 'HB(98).jpg',\n",
       " 'HB(99).jpg',\n",
       " 'HB(100).jpg',\n",
       " 'HB(101).jpg',\n",
       " 'HB(102).jpg',\n",
       " 'HB(103).jpg',\n",
       " 'HB(104).jpg',\n",
       " 'HB(105).jpg',\n",
       " 'HB(106).jpg',\n",
       " 'HB(107).jpg',\n",
       " 'HB(108).jpg',\n",
       " 'HB(109).jpg',\n",
       " 'HB(110).jpg',\n",
       " 'HB(111).jpg',\n",
       " 'HB(112).jpg',\n",
       " 'HB(113).jpg',\n",
       " 'HB(114).jpg',\n",
       " 'HB(115).jpg',\n",
       " 'HB(116).jpg',\n",
       " 'HB(117).jpg',\n",
       " 'HB(118).jpg',\n",
       " 'HB(119).jpg',\n",
       " 'HB(120).jpg',\n",
       " 'HB(121).jpg',\n",
       " 'HB(122).jpg',\n",
       " 'HB(123).jpg',\n",
       " 'HB(124).jpg',\n",
       " 'HB(125).jpg',\n",
       " 'HB(126).jpg',\n",
       " 'HB(127).jpg',\n",
       " 'HB(128).jpg',\n",
       " 'HB(129).jpg',\n",
       " 'HB(130).jpg',\n",
       " 'HB(131).jpg',\n",
       " 'HB(132).jpg',\n",
       " 'HB(133).jpg',\n",
       " 'HB(134).jpg',\n",
       " 'HB(135).jpg',\n",
       " 'HB(136).jpg',\n",
       " 'HB(137).jpg',\n",
       " 'HB(138).jpg',\n",
       " 'HB(139).jpg',\n",
       " 'HB(140).jpg',\n",
       " 'HB(141).jpg',\n",
       " 'HB(142).jpg',\n",
       " 'HB(143).jpg',\n",
       " 'HB(144).jpg',\n",
       " 'HB(145).jpg',\n",
       " 'HB(146).jpg',\n",
       " 'HB(147).jpg',\n",
       " 'HB(148).jpg',\n",
       " 'HB(149).jpg',\n",
       " 'HB(150).jpg',\n",
       " 'HB(151).jpg',\n",
       " 'HB(152).jpg',\n",
       " 'HB(153).jpg',\n",
       " 'HB(154).jpg',\n",
       " 'HB(155).jpg',\n",
       " 'HB(156).jpg',\n",
       " 'HB(157).jpg',\n",
       " 'HB(158).jpg',\n",
       " 'HB(159).jpg',\n",
       " 'HB(160).jpg',\n",
       " 'HB(161).jpg',\n",
       " 'HB(162).jpg',\n",
       " 'HB(163).jpg',\n",
       " 'HB(164).jpg',\n",
       " 'HB(165).jpg',\n",
       " 'HB(166).jpg',\n",
       " 'HB(167).jpg',\n",
       " 'HB(168).jpg',\n",
       " 'HB(169).jpg',\n",
       " 'HB(170).jpg',\n",
       " 'HB(171).jpg',\n",
       " 'HB(172).jpg',\n",
       " 'HB(173).jpg',\n",
       " 'HB(174).jpg',\n",
       " 'HB(175).jpg',\n",
       " 'HB(176).jpg',\n",
       " 'HB(177).jpg',\n",
       " 'HB(178).jpg',\n",
       " 'HB(179).jpg',\n",
       " 'HB(180).jpg',\n",
       " 'HB(181).jpg',\n",
       " 'HB(182).jpg',\n",
       " 'HB(183).jpg',\n",
       " 'HB(184).jpg',\n",
       " 'HB(185).jpg',\n",
       " 'HB(186).jpg',\n",
       " 'HB(187).jpg',\n",
       " 'HB(188).jpg',\n",
       " 'HB(189).jpg',\n",
       " 'HB(190).jpg',\n",
       " 'HB(191).jpg',\n",
       " 'HB(192).jpg',\n",
       " 'HB(193).jpg',\n",
       " 'HB(194).jpg',\n",
       " 'HB(195).jpg',\n",
       " 'HB(196).jpg',\n",
       " 'HB(197).jpg',\n",
       " 'HB(198).jpg',\n",
       " 'HB(199).jpg',\n",
       " 'HB(200).jpg',\n",
       " 'HB(201).jpg',\n",
       " 'HB(202).jpg',\n",
       " 'HB(203).jpg',\n",
       " 'HB(204).jpg',\n",
       " 'HB(205).jpg',\n",
       " 'HB(206).jpg',\n",
       " 'HB(207).jpg',\n",
       " 'HB(208).jpg',\n",
       " 'HB(209).jpg',\n",
       " 'HB(210).jpg',\n",
       " 'HB(211).jpg',\n",
       " 'HB(212).jpg',\n",
       " 'HB(213).jpg',\n",
       " 'HB(214).jpg',\n",
       " 'HB(215).jpg',\n",
       " 'HB(216).jpg',\n",
       " 'HB(217).jpg',\n",
       " 'HB(218).jpg',\n",
       " 'HB(219).jpg',\n",
       " 'HB(220).jpg',\n",
       " 'HB(221).jpg',\n",
       " 'HB(222).jpg',\n",
       " 'HB(223).jpg',\n",
       " 'HB(224).jpg',\n",
       " 'HB(225).jpg',\n",
       " 'HB(226).jpg',\n",
       " 'HB(227).jpg',\n",
       " 'HB(228).jpg',\n",
       " 'HB(229).jpg',\n",
       " 'HB(230).jpg',\n",
       " 'HB(231).jpg',\n",
       " 'HB(232).jpg',\n",
       " 'HB(233).jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HB_ = natsorted(HB_)\n",
    "HB_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T4Ay50ch6Ru"
   },
   "source": [
    "#### **COMBINED CSV OF EACH LEAD(1-12) FROM ALL IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyTYwqJSX8Vh",
    "outputId": "b1c922e9-83f4-4d31-e5cd-d5b18a78acf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'HB', 'MI', 'PM'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now reading just lead1\n",
    "df=pd.read_csv('Combined1d_csv/Combined_IDLead_1.csv')\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nbSKpUc2angF"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "fX_uYtekhL_z",
    "outputId": "a6d14a8e-a75f-4b1b-f39f-7830cba4c581"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728449</td>\n",
       "      <td>0.680755</td>\n",
       "      <td>0.619010</td>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.681570</td>\n",
       "      <td>0.732488</td>\n",
       "      <td>0.758448</td>\n",
       "      <td>0.750660</td>\n",
       "      <td>0.728282</td>\n",
       "      <td>0.707928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637260</td>\n",
       "      <td>0.664539</td>\n",
       "      <td>0.667226</td>\n",
       "      <td>0.637064</td>\n",
       "      <td>0.593287</td>\n",
       "      <td>0.545503</td>\n",
       "      <td>0.515049</td>\n",
       "      <td>0.563257</td>\n",
       "      <td>0.633581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957972</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>0.941024</td>\n",
       "      <td>0.930501</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>0.892244</td>\n",
       "      <td>0.868016</td>\n",
       "      <td>0.855127</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.798640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778790</td>\n",
       "      <td>0.806883</td>\n",
       "      <td>0.818640</td>\n",
       "      <td>0.842472</td>\n",
       "      <td>0.866740</td>\n",
       "      <td>0.884152</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.911293</td>\n",
       "      <td>0.922903</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611084</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>0.695790</td>\n",
       "      <td>0.741113</td>\n",
       "      <td>0.716666</td>\n",
       "      <td>0.595794</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.286457</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.611384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042690</td>\n",
       "      <td>0.165850</td>\n",
       "      <td>0.363445</td>\n",
       "      <td>0.549460</td>\n",
       "      <td>0.539346</td>\n",
       "      <td>0.522272</td>\n",
       "      <td>0.491668</td>\n",
       "      <td>0.454949</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.861690</td>\n",
       "      <td>0.866457</td>\n",
       "      <td>0.865756</td>\n",
       "      <td>0.855027</td>\n",
       "      <td>0.855606</td>\n",
       "      <td>0.845561</td>\n",
       "      <td>0.843187</td>\n",
       "      <td>0.846784</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789156</td>\n",
       "      <td>0.793622</td>\n",
       "      <td>0.787665</td>\n",
       "      <td>0.794515</td>\n",
       "      <td>0.796739</td>\n",
       "      <td>0.804063</td>\n",
       "      <td>0.809944</td>\n",
       "      <td>0.801814</td>\n",
       "      <td>0.777322</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.917753</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>0.873765</td>\n",
       "      <td>0.791381</td>\n",
       "      <td>0.699513</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>0.446012</td>\n",
       "      <td>0.528910</td>\n",
       "      <td>0.634068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200676</td>\n",
       "      <td>0.300147</td>\n",
       "      <td>0.407225</td>\n",
       "      <td>0.507346</td>\n",
       "      <td>0.605953</td>\n",
       "      <td>0.699309</td>\n",
       "      <td>0.790334</td>\n",
       "      <td>0.856593</td>\n",
       "      <td>0.849957</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.874246</td>\n",
       "      <td>0.877014</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.871349</td>\n",
       "      <td>0.912404</td>\n",
       "      <td>0.958148</td>\n",
       "      <td>0.977826</td>\n",
       "      <td>0.956314</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908312</td>\n",
       "      <td>0.926328</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.855709</td>\n",
       "      <td>0.823132</td>\n",
       "      <td>0.815458</td>\n",
       "      <td>0.818083</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429721</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.642137</td>\n",
       "      <td>0.742063</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.777622</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.759294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.417983</td>\n",
       "      <td>0.362322</td>\n",
       "      <td>0.351995</td>\n",
       "      <td>0.391493</td>\n",
       "      <td>0.418305</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.444598</td>\n",
       "      <td>0.460402</td>\n",
       "      <td>0.506810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408587</td>\n",
       "      <td>0.401864</td>\n",
       "      <td>0.387069</td>\n",
       "      <td>0.359590</td>\n",
       "      <td>0.325879</td>\n",
       "      <td>0.288894</td>\n",
       "      <td>0.293521</td>\n",
       "      <td>0.344504</td>\n",
       "      <td>0.399012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.682510</td>\n",
       "      <td>0.682286</td>\n",
       "      <td>0.641051</td>\n",
       "      <td>0.620212</td>\n",
       "      <td>0.608210</td>\n",
       "      <td>0.576331</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.677964</td>\n",
       "      <td>0.720297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452247</td>\n",
       "      <td>0.450421</td>\n",
       "      <td>0.439278</td>\n",
       "      <td>0.439086</td>\n",
       "      <td>0.394417</td>\n",
       "      <td>0.441650</td>\n",
       "      <td>0.473909</td>\n",
       "      <td>0.539199</td>\n",
       "      <td>0.547146</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.792175</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>0.820559</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.880933</td>\n",
       "      <td>0.902061</td>\n",
       "      <td>0.878266</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737351</td>\n",
       "      <td>0.778845</td>\n",
       "      <td>0.805446</td>\n",
       "      <td>0.782640</td>\n",
       "      <td>0.751236</td>\n",
       "      <td>0.741331</td>\n",
       "      <td>0.718790</td>\n",
       "      <td>0.714504</td>\n",
       "      <td>0.691004</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.728449  0.680755  0.619010  0.645367  0.681570  0.732488  0.758448   \n",
       "1    0.957972  0.950695  0.941024  0.930501  0.913601  0.892244  0.868016   \n",
       "2    0.611084  0.661575  0.695790  0.741113  0.716666  0.595794  0.425022   \n",
       "3    0.839213  0.861690  0.866457  0.865756  0.855027  0.855606  0.845561   \n",
       "4    0.917753  0.924369  0.873765  0.791381  0.699513  0.604927  0.500312   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.874246  0.877014  0.864280  0.860505  0.871349  0.912404  0.958148   \n",
       "924  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "925  0.469048  0.417983  0.362322  0.351995  0.391493  0.418305  0.440135   \n",
       "926  0.682510  0.682286  0.641051  0.620212  0.608210  0.576331  0.603596   \n",
       "927  0.792175  0.815695  0.819518  0.820559  0.847985  0.880933  0.902061   \n",
       "\n",
       "            7         8         9  ...       246       247       248  \\\n",
       "0    0.750660  0.728282  0.707928  ...  0.637260  0.664539  0.667226   \n",
       "1    0.855127  0.835307  0.798640  ...  0.778790  0.806883  0.818640   \n",
       "2    0.286457  0.425022  0.611384  ...  0.000000  0.042690  0.165850   \n",
       "3    0.843187  0.846784  0.824438  ...  0.789156  0.793622  0.787665   \n",
       "4    0.446012  0.528910  0.634068  ...  0.200676  0.300147  0.407225   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.977826  0.956314  0.926773  ...  0.908312  0.926328  0.898749   \n",
       "924  0.821865  0.721302  0.612039  ...  0.429721  0.531567  0.642137   \n",
       "925  0.444598  0.460402  0.506810  ...  0.408587  0.401864  0.387069   \n",
       "926  0.645714  0.677964  0.720297  ...  0.452247  0.450421  0.439278   \n",
       "927  0.878266  0.838806  0.811795  ...  0.737351  0.778845  0.805446   \n",
       "\n",
       "          249       250       251       252       253       254  target  \n",
       "0    0.637064  0.593287  0.545503  0.515049  0.563257  0.633581       2  \n",
       "1    0.842472  0.866740  0.884152  0.897196  0.911293  0.922903       2  \n",
       "2    0.363445  0.549460  0.539346  0.522272  0.491668  0.454949       2  \n",
       "3    0.794515  0.796739  0.804063  0.809944  0.801814  0.777322       2  \n",
       "4    0.507346  0.605953  0.699309  0.790334  0.856593  0.849957       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.855709  0.823132  0.815458  0.818083  0.829300  0.822382       3  \n",
       "924  0.742063  0.833042  0.814867  0.777622  0.760714  0.759294       3  \n",
       "925  0.359590  0.325879  0.288894  0.293521  0.344504  0.399012       3  \n",
       "926  0.439086  0.394417  0.441650  0.473909  0.539199  0.547146       3  \n",
       "927  0.782640  0.751236  0.741331  0.718790  0.714504  0.691004       3  \n",
       "\n",
       "[928 rows x 256 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert Target column values as Numeric using ngroups\n",
    "encode_target_label = df.groupby('Target').ngroup().rename(\"target\").to_frame()\n",
    "test_final  = df.merge(encode_target_label, left_index=True, right_index=True)\n",
    "test_final.drop(columns=['Target'],inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRvRDvfrivtE"
   },
   "source": [
    "#### **PERFORM DIMENSIONALITY REDUCTION JUST FOR CHECKING/UNDERSTANDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "scVPRQ3TZBaW",
    "outputId": "0b606399-2d4a-40a0-b9e3-cd7592305d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [1.76145888e-01 9.50265614e-02 6.99060614e-02 6.15960001e-02\n",
      " 5.34876630e-02 4.23664893e-02 3.68320213e-02 3.38541791e-02\n",
      " 3.00884979e-02 2.90396728e-02 2.64962509e-02 2.42272738e-02\n",
      " 2.10221030e-02 1.99751559e-02 1.77321042e-02 1.63016802e-02\n",
      " 1.53898622e-02 1.48412074e-02 1.33644825e-02 1.19674074e-02\n",
      " 1.16813409e-02 1.05807650e-02 9.68875480e-03 9.47385060e-03\n",
      " 8.65347748e-03 8.47506998e-03 7.93382172e-03 7.30163338e-03\n",
      " 6.76380665e-03 6.36886390e-03 6.02004791e-03 5.46823032e-03\n",
      " 5.31229911e-03 4.97821789e-03 4.74686092e-03 4.46081684e-03\n",
      " 4.21254684e-03 4.01200243e-03 3.87246476e-03 3.52519084e-03\n",
      " 3.37596894e-03 3.26978336e-03 3.08241145e-03 2.96423495e-03\n",
      " 2.73419816e-03 2.50965698e-03 2.35335480e-03 2.25665349e-03\n",
      " 2.20141761e-03 1.96782025e-03 1.74343954e-03 1.70982830e-03\n",
      " 1.57456047e-03 1.53704487e-03 1.36768435e-03 1.33167096e-03\n",
      " 1.26444173e-03 1.20053330e-03 1.18738749e-03 1.08864087e-03\n",
      " 1.02824532e-03 9.11484783e-04 7.89962329e-04 7.59785111e-04\n",
      " 6.49920865e-04 6.27833793e-04 6.04784065e-04 5.45886709e-04\n",
      " 5.32310105e-04 4.97728350e-04 4.78393197e-04 4.50404968e-04\n",
      " 4.26173490e-04 4.09392362e-04 3.92601458e-04 3.70241594e-04\n",
      " 3.66854850e-04 3.38846661e-04 3.08205832e-04 3.00634179e-04\n",
      " 2.80363581e-04 2.67850942e-04 2.49661145e-04 2.40638856e-04\n",
      " 2.11563868e-04 2.03938831e-04 1.99025147e-04 1.83142197e-04\n",
      " 1.66818611e-04 1.65884848e-04 1.55970232e-04 1.39160416e-04\n",
      " 1.25331993e-04 1.22319633e-04 1.13621266e-04 1.09455775e-04\n",
      " 1.03401423e-04 9.93335981e-05 9.86624050e-05 9.42062716e-05]\n",
      "\n",
      " Total Variance Explained: 99.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018578</td>\n",
       "      <td>1.148263</td>\n",
       "      <td>-0.589582</td>\n",
       "      <td>0.193617</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.309400</td>\n",
       "      <td>-0.161566</td>\n",
       "      <td>0.478471</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>-0.031832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>-0.003519</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.013465</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.061380</td>\n",
       "      <td>-0.023797</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.098692</td>\n",
       "      <td>0.289832</td>\n",
       "      <td>-1.766388</td>\n",
       "      <td>1.076165</td>\n",
       "      <td>-0.261201</td>\n",
       "      <td>-0.820446</td>\n",
       "      <td>-0.474188</td>\n",
       "      <td>-0.515238</td>\n",
       "      <td>0.692389</td>\n",
       "      <td>1.501606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017575</td>\n",
       "      <td>0.060785</td>\n",
       "      <td>-0.007171</td>\n",
       "      <td>0.013413</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.034263</td>\n",
       "      <td>-0.033939</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275021</td>\n",
       "      <td>-0.451289</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>-0.426415</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.692474</td>\n",
       "      <td>0.634894</td>\n",
       "      <td>-0.035867</td>\n",
       "      <td>0.815855</td>\n",
       "      <td>-0.909473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045854</td>\n",
       "      <td>-0.042525</td>\n",
       "      <td>-0.031407</td>\n",
       "      <td>0.119175</td>\n",
       "      <td>-0.044535</td>\n",
       "      <td>-0.018434</td>\n",
       "      <td>-0.021134</td>\n",
       "      <td>0.047169</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.517085</td>\n",
       "      <td>1.662693</td>\n",
       "      <td>-1.021167</td>\n",
       "      <td>0.804267</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>0.518180</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>-0.344235</td>\n",
       "      <td>-0.910867</td>\n",
       "      <td>-0.629517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.042049</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>-0.051191</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>-0.016082</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>-0.034789</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.152840</td>\n",
       "      <td>-1.046283</td>\n",
       "      <td>0.351278</td>\n",
       "      <td>1.100381</td>\n",
       "      <td>-1.613642</td>\n",
       "      <td>1.484188</td>\n",
       "      <td>-0.113277</td>\n",
       "      <td>-0.251152</td>\n",
       "      <td>0.179023</td>\n",
       "      <td>-0.233104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009427</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.031338</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>-0.006434</td>\n",
       "      <td>0.040720</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>-0.023691</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-1.321884</td>\n",
       "      <td>2.153021</td>\n",
       "      <td>0.788596</td>\n",
       "      <td>-1.304253</td>\n",
       "      <td>0.458186</td>\n",
       "      <td>-0.859346</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.392796</td>\n",
       "      <td>0.755732</td>\n",
       "      <td>-0.584050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036490</td>\n",
       "      <td>0.077101</td>\n",
       "      <td>-0.044587</td>\n",
       "      <td>-0.032999</td>\n",
       "      <td>0.009659</td>\n",
       "      <td>-0.023058</td>\n",
       "      <td>-0.048360</td>\n",
       "      <td>-0.010403</td>\n",
       "      <td>0.008708</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.867163</td>\n",
       "      <td>-0.040504</td>\n",
       "      <td>0.940680</td>\n",
       "      <td>0.302648</td>\n",
       "      <td>-0.469672</td>\n",
       "      <td>-0.368255</td>\n",
       "      <td>1.065579</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>-0.092878</td>\n",
       "      <td>0.013424</td>\n",
       "      <td>-0.029291</td>\n",
       "      <td>-0.008844</td>\n",
       "      <td>0.057958</td>\n",
       "      <td>-0.006966</td>\n",
       "      <td>-0.064884</td>\n",
       "      <td>-0.021760</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>3.753012</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>-0.317393</td>\n",
       "      <td>-0.296117</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>-0.255474</td>\n",
       "      <td>-0.057091</td>\n",
       "      <td>-0.072048</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>-0.837668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008957</td>\n",
       "      <td>-0.006074</td>\n",
       "      <td>-0.003050</td>\n",
       "      <td>0.006474</td>\n",
       "      <td>-0.024333</td>\n",
       "      <td>0.067133</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>-0.003514</td>\n",
       "      <td>-0.007296</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.126259</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.283612</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.156326</td>\n",
       "      <td>-0.068399</td>\n",
       "      <td>-0.184308</td>\n",
       "      <td>0.461063</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056040</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>-0.014552</td>\n",
       "      <td>-0.079017</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.037042</td>\n",
       "      <td>-0.053916</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.065322</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.452945</td>\n",
       "      <td>1.233599</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>0.165928</td>\n",
       "      <td>-0.171830</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>-0.687583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012395</td>\n",
       "      <td>-0.031875</td>\n",
       "      <td>0.070871</td>\n",
       "      <td>-0.062704</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-0.076223</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>-0.055386</td>\n",
       "      <td>-0.026809</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
       "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
       "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
       "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
       "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
       "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
       "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
       "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
       "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0    0.478471  0.972403 -0.031832  ...  0.012173 -0.003519  0.023545   \n",
       "1   -0.515238  0.692389  1.501606  ... -0.017575  0.060785 -0.007171   \n",
       "2   -0.035867  0.815855 -0.909473  ...  0.045854 -0.042525 -0.031407   \n",
       "3   -0.344235 -0.910867 -0.629517  ...  0.001174  0.042049  0.018082   \n",
       "4   -0.251152  0.179023 -0.233104  ... -0.009427  0.055497  0.003259   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923 -0.392796  0.755732 -0.584050  ... -0.036490  0.077101 -0.044587   \n",
       "924  0.801522  0.690113  0.953008  ...  0.012195 -0.092878  0.013424   \n",
       "925 -0.072048  0.664386 -0.837668  ... -0.008957 -0.006074 -0.003050   \n",
       "926 -0.184308  0.461063 -0.002047  ...  0.056040  0.023661 -0.014552   \n",
       "927  0.033859  0.004444 -0.687583  ...  0.012395 -0.031875  0.070871   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0    0.013465  0.000690  0.003673  0.006348 -0.061380 -0.023797       2  \n",
       "1    0.013413  0.000601  0.044646  0.034263 -0.033939 -0.000081       2  \n",
       "2    0.119175 -0.044535 -0.018434 -0.021134  0.047169  0.002745       2  \n",
       "3   -0.051191 -0.041015 -0.016082 -0.023625  0.024546 -0.034789       2  \n",
       "4    0.031338  0.016204 -0.006434  0.040720  0.002520 -0.023691       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.032999  0.009659 -0.023058 -0.048360 -0.010403  0.008708       3  \n",
       "924 -0.029291 -0.008844  0.057958 -0.006966 -0.064884 -0.021760       3  \n",
       "925  0.006474 -0.024333  0.067133  0.020665 -0.003514 -0.007296       3  \n",
       "926 -0.079017 -0.005191  0.037042 -0.053916  0.020761  0.065322       3  \n",
       "927 -0.062704  0.012071 -0.076223  0.002564 -0.055386 -0.026809       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just for testing\n",
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 100\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(test_final.iloc[:,0:-1])\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(test_final['target'], name='target')\n",
    "result_df = pd.concat([pca_df, target], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "ErMVKLMrKBiJ",
    "outputId": "f7ccdbdd-773b-4a0e-d7de-d467d120a211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018578</td>\n",
       "      <td>1.148263</td>\n",
       "      <td>-0.589582</td>\n",
       "      <td>0.193617</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.309400</td>\n",
       "      <td>-0.161566</td>\n",
       "      <td>0.478471</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>-0.031832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>-0.003519</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.013465</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.061380</td>\n",
       "      <td>-0.023797</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.098692</td>\n",
       "      <td>0.289832</td>\n",
       "      <td>-1.766388</td>\n",
       "      <td>1.076165</td>\n",
       "      <td>-0.261201</td>\n",
       "      <td>-0.820446</td>\n",
       "      <td>-0.474188</td>\n",
       "      <td>-0.515238</td>\n",
       "      <td>0.692389</td>\n",
       "      <td>1.501606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017575</td>\n",
       "      <td>0.060785</td>\n",
       "      <td>-0.007171</td>\n",
       "      <td>0.013413</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.034263</td>\n",
       "      <td>-0.033939</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275021</td>\n",
       "      <td>-0.451289</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>-0.426415</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.692474</td>\n",
       "      <td>0.634894</td>\n",
       "      <td>-0.035867</td>\n",
       "      <td>0.815855</td>\n",
       "      <td>-0.909473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045854</td>\n",
       "      <td>-0.042525</td>\n",
       "      <td>-0.031407</td>\n",
       "      <td>0.119175</td>\n",
       "      <td>-0.044535</td>\n",
       "      <td>-0.018434</td>\n",
       "      <td>-0.021134</td>\n",
       "      <td>0.047169</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.517085</td>\n",
       "      <td>1.662693</td>\n",
       "      <td>-1.021167</td>\n",
       "      <td>0.804267</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>0.518180</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>-0.344235</td>\n",
       "      <td>-0.910867</td>\n",
       "      <td>-0.629517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.042049</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>-0.051191</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>-0.016082</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>-0.034789</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.152840</td>\n",
       "      <td>-1.046283</td>\n",
       "      <td>0.351278</td>\n",
       "      <td>1.100381</td>\n",
       "      <td>-1.613642</td>\n",
       "      <td>1.484188</td>\n",
       "      <td>-0.113277</td>\n",
       "      <td>-0.251152</td>\n",
       "      <td>0.179023</td>\n",
       "      <td>-0.233104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009427</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.031338</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>-0.006434</td>\n",
       "      <td>0.040720</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>-0.023691</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-1.321884</td>\n",
       "      <td>2.153021</td>\n",
       "      <td>0.788596</td>\n",
       "      <td>-1.304253</td>\n",
       "      <td>0.458186</td>\n",
       "      <td>-0.859346</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.392796</td>\n",
       "      <td>0.755732</td>\n",
       "      <td>-0.584050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036490</td>\n",
       "      <td>0.077101</td>\n",
       "      <td>-0.044587</td>\n",
       "      <td>-0.032999</td>\n",
       "      <td>0.009659</td>\n",
       "      <td>-0.023058</td>\n",
       "      <td>-0.048360</td>\n",
       "      <td>-0.010403</td>\n",
       "      <td>0.008708</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.867163</td>\n",
       "      <td>-0.040504</td>\n",
       "      <td>0.940680</td>\n",
       "      <td>0.302648</td>\n",
       "      <td>-0.469672</td>\n",
       "      <td>-0.368255</td>\n",
       "      <td>1.065579</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>-0.092878</td>\n",
       "      <td>0.013424</td>\n",
       "      <td>-0.029291</td>\n",
       "      <td>-0.008844</td>\n",
       "      <td>0.057958</td>\n",
       "      <td>-0.006966</td>\n",
       "      <td>-0.064884</td>\n",
       "      <td>-0.021760</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>3.753012</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>-0.317393</td>\n",
       "      <td>-0.296117</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>-0.255474</td>\n",
       "      <td>-0.057091</td>\n",
       "      <td>-0.072048</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>-0.837668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008957</td>\n",
       "      <td>-0.006074</td>\n",
       "      <td>-0.003050</td>\n",
       "      <td>0.006474</td>\n",
       "      <td>-0.024333</td>\n",
       "      <td>0.067133</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>-0.003514</td>\n",
       "      <td>-0.007296</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.126259</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.283612</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.156326</td>\n",
       "      <td>-0.068399</td>\n",
       "      <td>-0.184308</td>\n",
       "      <td>0.461063</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056040</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>-0.014552</td>\n",
       "      <td>-0.079017</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.037042</td>\n",
       "      <td>-0.053916</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.065322</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.452945</td>\n",
       "      <td>1.233599</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>0.165928</td>\n",
       "      <td>-0.171830</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>-0.687583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012395</td>\n",
       "      <td>-0.031875</td>\n",
       "      <td>0.070871</td>\n",
       "      <td>-0.062704</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-0.076223</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>-0.055386</td>\n",
       "      <td>-0.026809</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
       "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
       "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
       "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
       "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
       "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
       "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
       "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
       "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0    0.478471  0.972403 -0.031832  ...  0.012173 -0.003519  0.023545   \n",
       "1   -0.515238  0.692389  1.501606  ... -0.017575  0.060785 -0.007171   \n",
       "2   -0.035867  0.815855 -0.909473  ...  0.045854 -0.042525 -0.031407   \n",
       "3   -0.344235 -0.910867 -0.629517  ...  0.001174  0.042049  0.018082   \n",
       "4   -0.251152  0.179023 -0.233104  ... -0.009427  0.055497  0.003259   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923 -0.392796  0.755732 -0.584050  ... -0.036490  0.077101 -0.044587   \n",
       "924  0.801522  0.690113  0.953008  ...  0.012195 -0.092878  0.013424   \n",
       "925 -0.072048  0.664386 -0.837668  ... -0.008957 -0.006074 -0.003050   \n",
       "926 -0.184308  0.461063 -0.002047  ...  0.056040  0.023661 -0.014552   \n",
       "927  0.033859  0.004444 -0.687583  ...  0.012395 -0.031875  0.070871   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0    0.013465  0.000690  0.003673  0.006348 -0.061380 -0.023797       2  \n",
       "1    0.013413  0.000601  0.044646  0.034263 -0.033939 -0.000081       2  \n",
       "2    0.119175 -0.044535 -0.018434 -0.021134  0.047169  0.002745       2  \n",
       "3   -0.051191 -0.041015 -0.016082 -0.023625  0.024546 -0.034789       2  \n",
       "4    0.031338  0.016204 -0.006434  0.040720  0.002520 -0.023691       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.032999  0.009659 -0.023058 -0.048360 -0.010403  0.008708       3  \n",
       "924 -0.029291 -0.008844  0.057958 -0.006966 -0.064884 -0.021760       3  \n",
       "925  0.006474 -0.024333  0.067133  0.020665 -0.003514 -0.007296       3  \n",
       "926 -0.079017 -0.005191  0.037042 -0.053916  0.020761  0.065322       3  \n",
       "927 -0.062704  0.012071 -0.076223  0.002564 -0.055386 -0.026809       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1GNZtOUi6PP"
   },
   "source": [
    "#### **NOW COMBINING ALL 12 LEADS INTO A SINGLE CSV FILE AND THEN PERFROM MODEL ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "huNy0hsWSkr5",
    "outputId": "ae4fc5c3-8b46-4d0a-d075-c2c64559680c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3051</th>\n",
       "      <th>3052</th>\n",
       "      <th>3053</th>\n",
       "      <th>3054</th>\n",
       "      <th>3055</th>\n",
       "      <th>3056</th>\n",
       "      <th>3057</th>\n",
       "      <th>3058</th>\n",
       "      <th>3059</th>\n",
       "      <th>3060</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728449</td>\n",
       "      <td>0.680755</td>\n",
       "      <td>0.619010</td>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.681570</td>\n",
       "      <td>0.732488</td>\n",
       "      <td>0.758448</td>\n",
       "      <td>0.750660</td>\n",
       "      <td>0.728282</td>\n",
       "      <td>0.707928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864067</td>\n",
       "      <td>0.849256</td>\n",
       "      <td>0.854949</td>\n",
       "      <td>0.861380</td>\n",
       "      <td>0.875514</td>\n",
       "      <td>0.868763</td>\n",
       "      <td>0.847450</td>\n",
       "      <td>0.805689</td>\n",
       "      <td>0.751761</td>\n",
       "      <td>0.702102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957972</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>0.941024</td>\n",
       "      <td>0.930501</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>0.892244</td>\n",
       "      <td>0.868016</td>\n",
       "      <td>0.855127</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.798640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925865</td>\n",
       "      <td>0.928285</td>\n",
       "      <td>0.946033</td>\n",
       "      <td>0.947274</td>\n",
       "      <td>0.946394</td>\n",
       "      <td>0.936536</td>\n",
       "      <td>0.920869</td>\n",
       "      <td>0.910320</td>\n",
       "      <td>0.905436</td>\n",
       "      <td>0.876942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611084</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>0.695790</td>\n",
       "      <td>0.741113</td>\n",
       "      <td>0.716666</td>\n",
       "      <td>0.595794</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.286457</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.611384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170137</td>\n",
       "      <td>0.166206</td>\n",
       "      <td>0.207633</td>\n",
       "      <td>0.258184</td>\n",
       "      <td>0.286993</td>\n",
       "      <td>0.304742</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>0.361189</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>0.543373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.861690</td>\n",
       "      <td>0.866457</td>\n",
       "      <td>0.865756</td>\n",
       "      <td>0.855027</td>\n",
       "      <td>0.855606</td>\n",
       "      <td>0.845561</td>\n",
       "      <td>0.843187</td>\n",
       "      <td>0.846784</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880043</td>\n",
       "      <td>0.883833</td>\n",
       "      <td>0.870995</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.864892</td>\n",
       "      <td>0.863552</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.805486</td>\n",
       "      <td>0.801828</td>\n",
       "      <td>0.826618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.917753</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>0.873765</td>\n",
       "      <td>0.791381</td>\n",
       "      <td>0.699513</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>0.446012</td>\n",
       "      <td>0.528910</td>\n",
       "      <td>0.634068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400774</td>\n",
       "      <td>0.380920</td>\n",
       "      <td>0.439510</td>\n",
       "      <td>0.505257</td>\n",
       "      <td>0.561538</td>\n",
       "      <td>0.577997</td>\n",
       "      <td>0.566082</td>\n",
       "      <td>0.547642</td>\n",
       "      <td>0.538735</td>\n",
       "      <td>0.527560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.874246</td>\n",
       "      <td>0.877014</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.871349</td>\n",
       "      <td>0.912404</td>\n",
       "      <td>0.958148</td>\n",
       "      <td>0.977826</td>\n",
       "      <td>0.956314</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785103</td>\n",
       "      <td>0.676795</td>\n",
       "      <td>0.579054</td>\n",
       "      <td>0.476613</td>\n",
       "      <td>0.458748</td>\n",
       "      <td>0.565470</td>\n",
       "      <td>0.681896</td>\n",
       "      <td>0.792646</td>\n",
       "      <td>0.871660</td>\n",
       "      <td>0.872789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913404</td>\n",
       "      <td>0.968015</td>\n",
       "      <td>0.992614</td>\n",
       "      <td>0.945789</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>0.808906</td>\n",
       "      <td>0.741645</td>\n",
       "      <td>0.736615</td>\n",
       "      <td>0.797729</td>\n",
       "      <td>0.855637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.417983</td>\n",
       "      <td>0.362322</td>\n",
       "      <td>0.351995</td>\n",
       "      <td>0.391493</td>\n",
       "      <td>0.418305</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.444598</td>\n",
       "      <td>0.460402</td>\n",
       "      <td>0.506810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730354</td>\n",
       "      <td>0.697465</td>\n",
       "      <td>0.714527</td>\n",
       "      <td>0.745605</td>\n",
       "      <td>0.754952</td>\n",
       "      <td>0.755059</td>\n",
       "      <td>0.755059</td>\n",
       "      <td>0.755093</td>\n",
       "      <td>0.759093</td>\n",
       "      <td>0.767555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.682510</td>\n",
       "      <td>0.682286</td>\n",
       "      <td>0.641051</td>\n",
       "      <td>0.620212</td>\n",
       "      <td>0.608210</td>\n",
       "      <td>0.576331</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.677964</td>\n",
       "      <td>0.720297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860939</td>\n",
       "      <td>0.824976</td>\n",
       "      <td>0.783459</td>\n",
       "      <td>0.761391</td>\n",
       "      <td>0.741917</td>\n",
       "      <td>0.770631</td>\n",
       "      <td>0.802701</td>\n",
       "      <td>0.821503</td>\n",
       "      <td>0.846300</td>\n",
       "      <td>0.858795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.792175</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>0.820559</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.880933</td>\n",
       "      <td>0.902061</td>\n",
       "      <td>0.878266</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516381</td>\n",
       "      <td>0.521133</td>\n",
       "      <td>0.521142</td>\n",
       "      <td>0.521143</td>\n",
       "      <td>0.522801</td>\n",
       "      <td>0.543166</td>\n",
       "      <td>0.549073</td>\n",
       "      <td>0.564977</td>\n",
       "      <td>0.576139</td>\n",
       "      <td>0.576267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows Ã— 3060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.728449  0.680755  0.619010  0.645367  0.681570  0.732488  0.758448   \n",
       "1    0.957972  0.950695  0.941024  0.930501  0.913601  0.892244  0.868016   \n",
       "2    0.611084  0.661575  0.695790  0.741113  0.716666  0.595794  0.425022   \n",
       "3    0.839213  0.861690  0.866457  0.865756  0.855027  0.855606  0.845561   \n",
       "4    0.917753  0.924369  0.873765  0.791381  0.699513  0.604927  0.500312   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.874246  0.877014  0.864280  0.860505  0.871349  0.912404  0.958148   \n",
       "924  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "925  0.469048  0.417983  0.362322  0.351995  0.391493  0.418305  0.440135   \n",
       "926  0.682510  0.682286  0.641051  0.620212  0.608210  0.576331  0.603596   \n",
       "927  0.792175  0.815695  0.819518  0.820559  0.847985  0.880933  0.902061   \n",
       "\n",
       "         7         8         9     ...      3051      3052      3053  \\\n",
       "0    0.750660  0.728282  0.707928  ...  0.864067  0.849256  0.854949   \n",
       "1    0.855127  0.835307  0.798640  ...  0.925865  0.928285  0.946033   \n",
       "2    0.286457  0.425022  0.611384  ...  0.170137  0.166206  0.207633   \n",
       "3    0.843187  0.846784  0.824438  ...  0.880043  0.883833  0.870995   \n",
       "4    0.446012  0.528910  0.634068  ...  0.400774  0.380920  0.439510   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.977826  0.956314  0.926773  ...  0.785103  0.676795  0.579054   \n",
       "924  0.821865  0.721302  0.612039  ...  0.913404  0.968015  0.992614   \n",
       "925  0.444598  0.460402  0.506810  ...  0.730354  0.697465  0.714527   \n",
       "926  0.645714  0.677964  0.720297  ...  0.860939  0.824976  0.783459   \n",
       "927  0.878266  0.838806  0.811795  ...  0.516381  0.521133  0.521142   \n",
       "\n",
       "         3054      3055      3056      3057      3058      3059      3060  \n",
       "0    0.861380  0.875514  0.868763  0.847450  0.805689  0.751761  0.702102  \n",
       "1    0.947274  0.946394  0.936536  0.920869  0.910320  0.905436  0.876942  \n",
       "2    0.258184  0.286993  0.304742  0.325659  0.361189  0.451946  0.543373  \n",
       "3    0.861323  0.864892  0.863552  0.839506  0.805486  0.801828  0.826618  \n",
       "4    0.505257  0.561538  0.577997  0.566082  0.547642  0.538735  0.527560  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "923  0.476613  0.458748  0.565470  0.681896  0.792646  0.871660  0.872789  \n",
       "924  0.945789  0.876660  0.808906  0.741645  0.736615  0.797729  0.855637  \n",
       "925  0.745605  0.754952  0.755059  0.755059  0.755093  0.759093  0.767555  \n",
       "926  0.761391  0.741917  0.770631  0.802701  0.821503  0.846300  0.858795  \n",
       "927  0.521143  0.522801  0.543166  0.549073  0.564977  0.576139  0.576267  \n",
       "\n",
       "[928 rows x 3060 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try combining all 12 leads in a single csv\n",
    "location= 'Combined1d_csv'\n",
    "for files in natsorted(os.listdir(location)):\n",
    "  if files.endswith(\".csv\") and not files.endswith(\"13.csv\"):\n",
    "    if files!='Combined_IDLead_1.csv':\n",
    "      df=pd.read_csv('Combined1d_csv/{}'.format(files))\n",
    "      df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "      test_final=pd.concat([test_final,df],axis=1,ignore_index=True)\n",
    "      test_final.drop(columns=test_final.columns[-1],axis=1,inplace=True)\n",
    "\n",
    "#drop the target column\n",
    "test_final.drop(columns=[255],axis=1,inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OZ6_Lg0180y3"
   },
   "outputs": [],
   "source": [
    "#write the final file to csv\n",
    "test_final.to_csv('final_1D.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxWK-X-qjde2"
   },
   "source": [
    "#### **TEST DIMENSIONALITY REDUCTION EXPLAINED VARIANCE ON  THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WXvZGdh5cxrL",
    "outputId": "63b7ea81-03df-43ae-b708-630b9ce6722f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [8.04649534e-02 4.68818003e-02 3.76212504e-02 2.94708618e-02\n",
      " 2.57031130e-02 2.32574514e-02 2.14376788e-02 2.04315151e-02\n",
      " 1.94482863e-02 1.79877408e-02 1.64766264e-02 1.53241665e-02\n",
      " 1.50689862e-02 1.41398267e-02 1.36330466e-02 1.33375324e-02\n",
      " 1.26355566e-02 1.25577001e-02 1.16968257e-02 1.11671338e-02\n",
      " 1.07975552e-02 1.06183806e-02 1.03402122e-02 1.01248410e-02\n",
      " 9.73197948e-03 9.25504395e-03 9.16367637e-03 8.76267060e-03\n",
      " 8.54270112e-03 8.20665462e-03 8.07642149e-03 7.90742343e-03\n",
      " 7.54929819e-03 7.21938018e-03 7.07604659e-03 6.89135251e-03\n",
      " 6.80575532e-03 6.71875790e-03 6.38252148e-03 6.33951897e-03\n",
      " 6.10254734e-03 5.94560955e-03 5.76371295e-03 5.71788829e-03\n",
      " 5.55354810e-03 5.42316932e-03 5.35640711e-03 5.08429353e-03\n",
      " 5.03302777e-03 4.96811576e-03 4.87696491e-03 4.63686128e-03\n",
      " 4.55349933e-03 4.45390625e-03 4.31579996e-03 4.28316592e-03\n",
      " 4.17213140e-03 4.12346241e-03 4.09072049e-03 3.99349122e-03\n",
      " 3.92129459e-03 3.81982060e-03 3.78116652e-03 3.73307150e-03\n",
      " 3.68894307e-03 3.55238746e-03 3.49148625e-03 3.40490507e-03\n",
      " 3.33593814e-03 3.25467389e-03 3.20023474e-03 3.14871964e-03\n",
      " 3.09091665e-03 3.07180393e-03 3.05651457e-03 2.95447952e-03\n",
      " 2.90507083e-03 2.84618700e-03 2.80939396e-03 2.76324718e-03\n",
      " 2.71487874e-03 2.68959207e-03 2.67378836e-03 2.62085254e-03\n",
      " 2.55991613e-03 2.53614502e-03 2.47015404e-03 2.45768102e-03\n",
      " 2.41851536e-03 2.39477316e-03 2.35560704e-03 2.29236345e-03\n",
      " 2.26928539e-03 2.24965527e-03 2.22764534e-03 2.19258829e-03\n",
      " 2.14654982e-03 2.09081474e-03 2.08656961e-03 2.04315332e-03\n",
      " 2.01191187e-03 1.99715030e-03 1.98092986e-03 1.93183566e-03\n",
      " 1.90133601e-03 1.86628808e-03 1.85847904e-03 1.79040117e-03\n",
      " 1.77318190e-03 1.76278440e-03 1.73682193e-03 1.70177712e-03\n",
      " 1.69142157e-03 1.66289246e-03 1.64192361e-03 1.62455779e-03\n",
      " 1.59836820e-03 1.57166872e-03 1.56017874e-03 1.55193712e-03\n",
      " 1.52130395e-03 1.50860404e-03 1.48563216e-03 1.45667689e-03\n",
      " 1.44862677e-03 1.43014707e-03 1.42443426e-03 1.39341888e-03\n",
      " 1.38941740e-03 1.38032166e-03 1.35292505e-03 1.33403513e-03\n",
      " 1.33300728e-03 1.31774024e-03 1.29238722e-03 1.24574072e-03\n",
      " 1.23408862e-03 1.21598644e-03 1.20568485e-03 1.19391143e-03\n",
      " 1.18690274e-03 1.16630751e-03 1.16159095e-03 1.14539199e-03\n",
      " 1.13634359e-03 1.11858663e-03 1.10460060e-03 1.08515359e-03\n",
      " 1.07679695e-03 1.06488284e-03 1.05861426e-03 1.04012565e-03\n",
      " 1.03222232e-03 1.02519590e-03 1.01169941e-03 9.96444257e-04\n",
      " 9.76134514e-04 9.61104386e-04 9.57134099e-04 9.48294848e-04\n",
      " 9.35386446e-04 9.29858628e-04 9.24107282e-04 9.20229599e-04\n",
      " 9.00136970e-04 8.84392791e-04 8.60041244e-04 8.58222437e-04\n",
      " 8.39586154e-04 8.34156616e-04 8.24745137e-04 8.19630377e-04\n",
      " 8.11755902e-04 8.09589697e-04 7.93351930e-04 7.83229226e-04\n",
      " 7.69323633e-04 7.62916710e-04 7.61217310e-04 7.49412461e-04\n",
      " 7.41978508e-04 7.32319449e-04 7.28386324e-04 7.15766463e-04\n",
      " 7.00416470e-04 6.92792928e-04 6.87860571e-04 6.77118996e-04\n",
      " 6.69195650e-04 6.62776506e-04 6.52787237e-04 6.41350808e-04\n",
      " 6.31671343e-04 6.25941688e-04 6.20986817e-04 6.12964320e-04\n",
      " 6.06757241e-04 6.00414979e-04 5.90442751e-04 5.85447566e-04\n",
      " 5.82053388e-04 5.72736727e-04 5.64768427e-04 5.62060875e-04\n",
      " 5.53942338e-04 5.47413376e-04 5.43815848e-04 5.39018247e-04\n",
      " 5.31538796e-04 5.21422265e-04 5.16620308e-04 5.13730677e-04\n",
      " 5.08883049e-04 5.04308685e-04 4.96238365e-04 4.91958416e-04\n",
      " 4.80055673e-04 4.74422582e-04 4.69414332e-04 4.65649131e-04\n",
      " 4.62052064e-04 4.58664175e-04 4.49131977e-04 4.46512859e-04\n",
      " 4.45747674e-04 4.36928351e-04 4.30056931e-04 4.24233409e-04\n",
      " 4.21656146e-04 4.20467962e-04 4.16760270e-04 4.15888840e-04\n",
      " 4.07286797e-04 4.03273137e-04 3.97207458e-04 3.91816958e-04\n",
      " 3.87932381e-04 3.83350757e-04 3.82143411e-04 3.79404865e-04\n",
      " 3.72582501e-04 3.63610205e-04 3.59554000e-04 3.56530336e-04\n",
      " 3.53235192e-04 3.50500789e-04 3.47735151e-04 3.42040359e-04\n",
      " 3.38310299e-04 3.37306491e-04 3.35511675e-04 3.28894994e-04\n",
      " 3.28218990e-04 3.24306987e-04 3.20401249e-04 3.14252992e-04\n",
      " 3.10208379e-04 3.06218844e-04 3.04335166e-04 3.01021034e-04\n",
      " 2.97566419e-04 2.95304845e-04 2.91254940e-04 2.89125003e-04\n",
      " 2.82693307e-04 2.76520980e-04 2.75268847e-04 2.74379579e-04\n",
      " 2.68266948e-04 2.68061652e-04 2.66099946e-04 2.64421212e-04\n",
      " 2.63834089e-04 2.57431279e-04 2.54138713e-04 2.51605418e-04\n",
      " 2.46905238e-04 2.43635676e-04 2.42082945e-04 2.40633421e-04\n",
      " 2.37784263e-04 2.37571620e-04 2.35490052e-04 2.31228839e-04\n",
      " 2.29666878e-04 2.22982996e-04 2.22296846e-04 2.19367135e-04\n",
      " 2.18709348e-04 2.16023981e-04 2.12841778e-04 2.10474475e-04\n",
      " 2.08846755e-04 2.05998328e-04 2.04967016e-04 2.02934989e-04\n",
      " 1.99807732e-04 1.96639235e-04 1.94996169e-04 1.93137357e-04\n",
      " 1.91195417e-04 1.89711030e-04 1.88307639e-04 1.84368623e-04\n",
      " 1.82566093e-04 1.80379221e-04 1.76609524e-04 1.75643734e-04\n",
      " 1.72427661e-04 1.71788368e-04 1.71183382e-04 1.69725110e-04\n",
      " 1.67455433e-04 1.65839399e-04 1.63727124e-04 1.59521668e-04\n",
      " 1.59157469e-04 1.57743592e-04 1.56401075e-04 1.54530206e-04\n",
      " 1.51815951e-04 1.51100925e-04 1.49354365e-04 1.45429116e-04\n",
      " 1.42824004e-04 1.41655660e-04 1.38464886e-04 1.37018296e-04\n",
      " 1.36519761e-04 1.35573986e-04 1.33891408e-04 1.32421845e-04\n",
      " 1.31174191e-04 1.30742262e-04 1.29584275e-04 1.25910104e-04\n",
      " 1.25085731e-04 1.23617016e-04 1.22225716e-04 1.21537985e-04\n",
      " 1.20746083e-04 1.18283607e-04 1.17284277e-04 1.15529029e-04\n",
      " 1.14174828e-04 1.13056880e-04 1.11474920e-04 1.10136338e-04\n",
      " 1.08684309e-04 1.08289017e-04 1.06995143e-04 1.04915109e-04\n",
      " 1.04060741e-04 1.02716542e-04 1.01960541e-04 1.00974406e-04\n",
      " 9.88202735e-05 9.81485089e-05 9.71839530e-05 9.52022874e-05\n",
      " 9.46389854e-05 9.40274573e-05 9.26130784e-05 9.04913310e-05\n",
      " 8.90607474e-05 8.78113524e-05 8.67908699e-05 8.58242137e-05\n",
      " 8.48710499e-05 8.38950230e-05 8.30019605e-05 8.15262944e-05\n",
      " 8.07301483e-05 8.05995984e-05 7.97449163e-05 7.83609554e-05\n",
      " 7.76594923e-05 7.66081802e-05 7.54134792e-05 7.52233218e-05\n",
      " 7.39220030e-05 7.26544820e-05 7.15837256e-05 7.13055273e-05\n",
      " 7.00025054e-05 6.90212229e-05 6.82844074e-05 6.78839100e-05\n",
      " 6.59336801e-05 6.38682783e-05 6.38126916e-05 6.25884084e-05\n",
      " 6.18802560e-05 6.10318305e-05 5.99653922e-05 5.94224479e-05\n",
      " 5.88271621e-05 5.85133407e-05 5.65731144e-05 5.63530817e-05]\n",
      "\n",
      " Total Variance Explained: 99.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.457260</td>\n",
       "      <td>0.433402</td>\n",
       "      <td>-0.951313</td>\n",
       "      <td>-0.118767</td>\n",
       "      <td>0.372034</td>\n",
       "      <td>1.917341</td>\n",
       "      <td>0.503248</td>\n",
       "      <td>-0.996278</td>\n",
       "      <td>-2.306744</td>\n",
       "      <td>-1.294890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.099039</td>\n",
       "      <td>-0.069254</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>0.146381</td>\n",
       "      <td>-0.006523</td>\n",
       "      <td>-0.191421</td>\n",
       "      <td>-0.047282</td>\n",
       "      <td>0.057482</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.482871</td>\n",
       "      <td>-3.923833</td>\n",
       "      <td>1.001672</td>\n",
       "      <td>1.151786</td>\n",
       "      <td>-0.105634</td>\n",
       "      <td>0.935446</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>1.340459</td>\n",
       "      <td>-4.633610</td>\n",
       "      <td>-0.483502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150170</td>\n",
       "      <td>-0.077324</td>\n",
       "      <td>0.016306</td>\n",
       "      <td>0.130534</td>\n",
       "      <td>-0.096241</td>\n",
       "      <td>-0.080488</td>\n",
       "      <td>0.038958</td>\n",
       "      <td>-0.057695</td>\n",
       "      <td>0.100084</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.524849</td>\n",
       "      <td>-0.064380</td>\n",
       "      <td>-2.177830</td>\n",
       "      <td>-0.744632</td>\n",
       "      <td>-3.160053</td>\n",
       "      <td>-2.079289</td>\n",
       "      <td>1.991373</td>\n",
       "      <td>2.629458</td>\n",
       "      <td>1.251710</td>\n",
       "      <td>1.656291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013455</td>\n",
       "      <td>0.047012</td>\n",
       "      <td>0.076918</td>\n",
       "      <td>-0.031177</td>\n",
       "      <td>0.115332</td>\n",
       "      <td>0.098625</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.072663</td>\n",
       "      <td>-0.072035</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.626337</td>\n",
       "      <td>-0.901579</td>\n",
       "      <td>-1.351431</td>\n",
       "      <td>-1.767117</td>\n",
       "      <td>0.870166</td>\n",
       "      <td>2.210025</td>\n",
       "      <td>3.710694</td>\n",
       "      <td>-1.478115</td>\n",
       "      <td>-2.406238</td>\n",
       "      <td>-3.754295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116162</td>\n",
       "      <td>-0.084882</td>\n",
       "      <td>0.115377</td>\n",
       "      <td>0.037491</td>\n",
       "      <td>0.026842</td>\n",
       "      <td>0.085295</td>\n",
       "      <td>0.061483</td>\n",
       "      <td>-0.033297</td>\n",
       "      <td>-0.142864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.863027</td>\n",
       "      <td>-2.805259</td>\n",
       "      <td>-0.428457</td>\n",
       "      <td>-0.482452</td>\n",
       "      <td>0.231498</td>\n",
       "      <td>1.977056</td>\n",
       "      <td>1.605510</td>\n",
       "      <td>-0.603866</td>\n",
       "      <td>-1.312170</td>\n",
       "      <td>3.553396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037160</td>\n",
       "      <td>-0.057421</td>\n",
       "      <td>-0.045944</td>\n",
       "      <td>0.272777</td>\n",
       "      <td>0.118642</td>\n",
       "      <td>0.158180</td>\n",
       "      <td>0.086212</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>-0.187815</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-3.365774</td>\n",
       "      <td>-2.530997</td>\n",
       "      <td>-0.781861</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>1.960806</td>\n",
       "      <td>2.319130</td>\n",
       "      <td>3.632739</td>\n",
       "      <td>2.072028</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>1.423024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063927</td>\n",
       "      <td>0.055971</td>\n",
       "      <td>0.033680</td>\n",
       "      <td>-0.068964</td>\n",
       "      <td>-0.264574</td>\n",
       "      <td>-0.073667</td>\n",
       "      <td>-0.044929</td>\n",
       "      <td>-0.098494</td>\n",
       "      <td>0.065690</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-6.340754</td>\n",
       "      <td>0.634164</td>\n",
       "      <td>-3.259888</td>\n",
       "      <td>2.894988</td>\n",
       "      <td>2.625952</td>\n",
       "      <td>-1.435685</td>\n",
       "      <td>1.570297</td>\n",
       "      <td>0.456509</td>\n",
       "      <td>-0.855480</td>\n",
       "      <td>0.523852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204950</td>\n",
       "      <td>0.184494</td>\n",
       "      <td>0.043322</td>\n",
       "      <td>0.028740</td>\n",
       "      <td>-0.253749</td>\n",
       "      <td>0.120806</td>\n",
       "      <td>-0.125321</td>\n",
       "      <td>-0.038103</td>\n",
       "      <td>0.085843</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>-2.106097</td>\n",
       "      <td>4.489429</td>\n",
       "      <td>4.740686</td>\n",
       "      <td>2.521037</td>\n",
       "      <td>1.810682</td>\n",
       "      <td>1.252919</td>\n",
       "      <td>-0.925497</td>\n",
       "      <td>2.413092</td>\n",
       "      <td>-0.426080</td>\n",
       "      <td>-2.335654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014097</td>\n",
       "      <td>-0.028914</td>\n",
       "      <td>0.130358</td>\n",
       "      <td>0.169670</td>\n",
       "      <td>0.090610</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>0.131986</td>\n",
       "      <td>0.119439</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-1.091978</td>\n",
       "      <td>7.063249</td>\n",
       "      <td>2.692234</td>\n",
       "      <td>-0.543705</td>\n",
       "      <td>-0.443376</td>\n",
       "      <td>1.028903</td>\n",
       "      <td>-1.749000</td>\n",
       "      <td>0.469497</td>\n",
       "      <td>-0.575796</td>\n",
       "      <td>-1.181286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.113446</td>\n",
       "      <td>0.161004</td>\n",
       "      <td>-0.116032</td>\n",
       "      <td>-0.112429</td>\n",
       "      <td>0.062990</td>\n",
       "      <td>0.017277</td>\n",
       "      <td>-0.033040</td>\n",
       "      <td>-0.051991</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>6.346374</td>\n",
       "      <td>-3.865858</td>\n",
       "      <td>6.209416</td>\n",
       "      <td>1.247192</td>\n",
       "      <td>0.112905</td>\n",
       "      <td>0.278156</td>\n",
       "      <td>-0.507665</td>\n",
       "      <td>-0.342197</td>\n",
       "      <td>-0.152731</td>\n",
       "      <td>-3.172417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.053781</td>\n",
       "      <td>-0.034251</td>\n",
       "      <td>-0.186509</td>\n",
       "      <td>-0.106318</td>\n",
       "      <td>-0.071035</td>\n",
       "      <td>0.101463</td>\n",
       "      <td>-0.153531</td>\n",
       "      <td>0.027584</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows Ã— 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.457260  0.433402 -0.951313 -0.118767  0.372034  1.917341  0.503248   \n",
       "1   -1.482871 -3.923833  1.001672  1.151786 -0.105634  0.935446  0.061538   \n",
       "2    6.524849 -0.064380 -2.177830 -0.744632 -3.160053 -2.079289  1.991373   \n",
       "3   -4.626337 -0.901579 -1.351431 -1.767117  0.870166  2.210025  3.710694   \n",
       "4   -3.863027 -2.805259 -0.428457 -0.482452  0.231498  1.977056  1.605510   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -3.365774 -2.530997 -0.781861  0.567793  1.960806  2.319130  3.632739   \n",
       "924 -6.340754  0.634164 -3.259888  2.894988  2.625952 -1.435685  1.570297   \n",
       "925 -2.106097  4.489429  4.740686  2.521037  1.810682  1.252919 -0.925497   \n",
       "926 -1.091978  7.063249  2.692234 -0.543705 -0.443376  1.028903 -1.749000   \n",
       "927  6.346374 -3.865858  6.209416  1.247192  0.112905  0.278156 -0.507665   \n",
       "\n",
       "            7         8         9  ...       391       392       393  \\\n",
       "0   -0.996278 -2.306744 -1.294890  ...  0.011138  0.099039 -0.069254   \n",
       "1    1.340459 -4.633610 -0.483502  ...  0.150170 -0.077324  0.016306   \n",
       "2    2.629458  1.251710  1.656291  ... -0.013455  0.047012  0.076918   \n",
       "3   -1.478115 -2.406238 -3.754295  ... -0.116162 -0.084882  0.115377   \n",
       "4   -0.603866 -1.312170  3.553396  ...  0.037160 -0.057421 -0.045944   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  2.072028  0.318600  1.423024  ... -0.063927  0.055971  0.033680   \n",
       "924  0.456509 -0.855480  0.523852  ...  0.204950  0.184494  0.043322   \n",
       "925  2.413092 -0.426080 -2.335654  ... -0.014097 -0.028914  0.130358   \n",
       "926  0.469497 -0.575796 -1.181286  ...  0.009976  0.113446  0.161004   \n",
       "927 -0.342197 -0.152731 -3.172417  ...  0.070100  0.053781 -0.034251   \n",
       "\n",
       "          394       395       396       397       398       399  target  \n",
       "0    0.003510  0.146381 -0.006523 -0.191421 -0.047282  0.057482       2  \n",
       "1    0.130534 -0.096241 -0.080488  0.038958 -0.057695  0.100084       2  \n",
       "2   -0.031177  0.115332  0.098625  0.012483  0.072663 -0.072035       2  \n",
       "3    0.037491  0.026842  0.085295  0.061483 -0.033297 -0.142864       2  \n",
       "4    0.272777  0.118642  0.158180  0.086212 -0.006443 -0.187815       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.068964 -0.264574 -0.073667 -0.044929 -0.098494  0.065690       3  \n",
       "924  0.028740 -0.253749  0.120806 -0.125321 -0.038103  0.085843       3  \n",
       "925  0.169670  0.090610 -0.004033  0.131986  0.119439  0.012917       3  \n",
       "926 -0.116032 -0.112429  0.062990  0.017277 -0.033040 -0.051991       3  \n",
       "927 -0.186509 -0.106318 -0.071035  0.101463 -0.153531  0.027584       3  \n",
       "\n",
       "[928 rows x 401 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 400\n",
    "pca = PCA(n_components=400)\n",
    "x_pca = pca.fit_transform(test_final)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(result_df.iloc[:,-1], name='target')\n",
    "final_result_df = pd.concat([pca_df, target], axis=1)\n",
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Rfdk0yXlO6rz"
   },
   "outputs": [],
   "source": [
    "#save to dimensionally reduced csv file\n",
    "final_result_df.to_csv(\"pca_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n18ljrh0KJB3",
    "outputId": "7a6f991c-c1e9-430e-c97c-a4d8c2542876"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCA_ECG.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "#save the PCA model\n",
    "joblib_file='PCA_ECG.pkl'\n",
    "joblib.dump(pca,joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l93cv3DXjw2A"
   },
   "source": [
    "#### **TRYING DIFFERENT ML MODELS ON THE ALL 12 LEADS COMBINED FILE WITHOUT DIMENSIONALITY REDUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "METf9MJdkESh"
   },
   "source": [
    "##### **LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3qBayEUskDfc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 00:13:09.265307: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-02-15 00:13:09.265401: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-02-15 00:13:17.702393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-02-15 00:13:17.702464: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-02-15 00:13:17.702495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-0MK0TTK): /proc/driver/nvidia/version does not exist\n",
      "2024-02-15 00:13:17.702879: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv2d. Consider increasing the input size. Received input shape [None, 400, 1, 1] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106/392054900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Build 2DCNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    305\u001b[0m           \u001b[0;34mf'One of the dimensions in the output is <= 0 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m           \u001b[0;34mf'due to downsampling in {self.name}. Consider '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d. Consider increasing the input size. Received input shape [None, 400, 1, 1] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, matthews_corrcoef, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "# Input\n",
    "X = final_result_df.iloc[:, :-1]\n",
    "\n",
    "# Target\n",
    "y = final_result_df.iloc[:, -1]\n",
    "\n",
    "# Initialize StratifiedShuffleSplit with train_size and test_size\n",
    "kfold = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lists to store fold information\n",
    "fold_info = []\n",
    "fold_results = []\n",
    "fold_confusion_matrices = []\n",
    "\n",
    "# Reshape X to 2D array (assuming X is tabular data)\n",
    "X = X.values.reshape(X.shape[0], X.shape[1], 1)  # Reshape to (number of samples, number of features, 1)\n",
    "\n",
    "# Loop through each fold in cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X, y), 1):\n",
    "    # Split data into train set and test set for the current fold\n",
    "    X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Scaling data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold.reshape(X_train_fold.shape[0], -1)).reshape(X_train_fold.shape)\n",
    "    X_test_fold_scaled = scaler.transform(X_test_fold.reshape(X_test_fold.shape[0], -1)).reshape(X_test_fold.shape)\n",
    "\n",
    "    # Build 2DCNN model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(X_train_fold.shape[1], X_train_fold.shape[2], 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = History()\n",
    "    model.fit(X_train_fold_scaled, y_train_fold, epochs=300, batch_size=32, validation_split=0.2, callbacks=[history])\n",
    "\n",
    "    # Evaluate the model on test set\n",
    "    loss, accuracy = model.evaluate(X_test_fold_scaled, y_test_fold)\n",
    "\n",
    "    # Predict the labels of the test set\n",
    "    y_pred_fold = np.argmax(model.predict(X_test_fold_scaled), axis=-1)\n",
    "\n",
    "    # Calculate metrics: AUC, MCC\n",
    "    y_pred_proba = model.predict(X_test_fold_scaled)\n",
    "    auc = roc_auc_score(y_test_fold, y_pred_proba, average='macro', multi_class='ovo')\n",
    "    mcc = matthews_corrcoef(y_test_fold, y_pred_fold)\n",
    "\n",
    "    # Create classification report for the current fold\n",
    "    report = classification_report(y_test_fold, y_pred_fold, digits=5)\n",
    "\n",
    "    # Save confusion matrix of the current fold\n",
    "    fold_conf_matrix = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "    fold_confusion_matrices.append(fold_conf_matrix)\n",
    "\n",
    "    # Save information of the fold to the list\n",
    "    fold_info.append((accuracy, auc, mcc, report))\n",
    "\n",
    "    # Save results of the train and test set of the current fold\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'mcc': mcc,\n",
    "        'report': report,\n",
    "        'confusion_matrix': fold_conf_matrix\n",
    "    })\n",
    "\n",
    "    # Print the results of the current fold\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"MCC: {mcc}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Plot accuracy and loss curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Save results of each fold to a file\n",
    "with open('fold_results_2dcnn.txt', 'w') as f:\n",
    "    for result in fold_results:\n",
    "        f.write(f\"Fold: {result['fold']}\\n\")\n",
    "        f.write(f\"Accuracy: {result['accuracy']}\\n\")\n",
    "        f.write(f\"AUC: {result['auc']}\\n\")\n",
    "        f.write(f\"MCC: {result['mcc']}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(result['report'])\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(str(result['confusion_matrix']))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"-\" * 50)\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "# Calculate average values for test results\n",
    "acc_avg_test = np.mean([result['accuracy'] for result in fold_results])\n",
    "auc_avg_test = np.mean([result['auc'] for result in fold_results])\n",
    "mcc_avg_test = np.mean([result['mcc'] for result in fold_results])\n",
    "\n",
    "# Write overall average test results to 'avg_results.txt'\n",
    "with open('avg_results_2dcnn.txt', 'w') as f:\n",
    "    f.write(\"Average Test Results Across Folds:\\n\")\n",
    "    f.write(f\"Average Test Accuracy: {acc_avg_test}\\n\")\n",
    "    f.write(f\"Average Test AUC: {auc_avg_test}\\n\")\n",
    "    f.write(f\"Average Test MCC: {mcc_avg_test}\\n\")\n",
    "    f.write(\"-\" * 50)\n",
    "    f.write(\"\\nAverage Test Classification Report:\\n\")\n",
    "\n",
    "    # Combine classification reports for all folds\n",
    "    combined_classification_reports = \"\"\n",
    "    for fold_info_item in fold_info:\n",
    "        combined_classification_reports += fold_info_item[3] + \"\\n\\n\"\n",
    "\n",
    "    f.write(combined_classification_reports)\n",
    "\n",
    "class_labels = ['Normal', 'Abnormal Heart beat', 'Myocardial infarction', 'History of Myocardial infarction']\n",
    "\n",
    "# Calculate average confusion matrix of 10 folds\n",
    "test_confusion_matrix_avg = np.mean(fold_confusion_matrices, axis=0)\n",
    "\n",
    "# Normalize the confusion matrix by dividing each value by the sum of its row\n",
    "normalized_avg_test_cm = test_confusion_matrix_avg / test_confusion_matrix_avg.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the average confusion matrix of 10 folds\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(normalized_avg_test_cm, annot=True, cmap=\"Blues\", fmt=\".5f\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title(\"Average Test Confusion Matrix\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOJDLUA6p5Xd"
   },
   "source": [
    "##### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yG0rz8crlJK5",
    "outputId": "a560d998-8fd8-4fe9-bac6-be22cda925b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9408602150537635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92        54\n",
      "           1       1.00      1.00      1.00        50\n",
      "           2       0.90      0.89      0.90        53\n",
      "           3       0.94      1.00      0.97        29\n",
      "\n",
      "    accuracy                           0.94       186\n",
      "   macro avg       0.94      0.95      0.94       186\n",
      "weighted avg       0.94      0.94      0.94       186\n",
      "\n",
      "Tuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqxwP-4vIbbm"
   },
   "source": [
    "### **XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4xWCZUjyIWoJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bzdU_OL7IY0U"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OY33WUdIapb",
    "outputId": "40e112ea-0807-47bc-a873-0bdeeca4f0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.946236559139785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91        54\n",
      "           1       1.00      1.00      1.00        50\n",
      "           2       0.96      0.92      0.94        53\n",
      "           3       0.88      1.00      0.94        29\n",
      "\n",
      "    accuracy                           0.95       186\n",
      "   macro avg       0.94      0.95      0.95       186\n",
      "weighted avg       0.95      0.95      0.95       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izUvTTDBp-Vy"
   },
   "source": [
    "#### **SAVING A VERY BASIC ML MODEL AND USING IT ON REALTIME PIPELINE TO CHECK WORKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mN7RIL_3PnJb",
    "outputId": "6bcad7b1-cfbc-4066-f8c2-5404e4e6da11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_model_test.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='knn_model_test.pkl'\n",
    "joblib.dump(knn,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNiBzgDv_-oA",
    "outputId": "e68d2bfc-1935-404e-de27-3628e2d9e0c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model_test.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "svm=SVC(C=10,gamma=0.01)\n",
    "\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='svm_model_test.pkl'\n",
    "joblib.dump(svm,joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmKRxfCBH7Yo"
   },
   "source": [
    "### **ENSEMBLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7V2P-WtiYZ4f"
   },
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "from sklearn import linear_model, tree, ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eA1TkdrcYZ4g"
   },
   "outputs": [],
   "source": [
    "#input\n",
    "X = final_result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "A4caey39YZ4f"
   },
   "outputs": [],
   "source": [
    "# Stacking of ML Models\n",
    "eclf = VotingClassifier(estimators=[ \n",
    "    ('SVM', SVC(probability=True)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('rf', ensemble.RandomForestClassifier()),\n",
    "    ('bayes',GaussianNB()),\n",
    "    ('logistic',LogisticRegression()),\n",
    "    ], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "neqxcy3GqS91",
    "outputId": "5acb921a-9719-4836-8ef2-4b7ba370e27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM__C': 1, 'SVM__gamma': 0.01, 'knn__n_neighbors': 5, 'rf__n_estimators': 400}\n",
      "Accuracy: 0.9408602150537635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        51\n",
      "           1       1.00      1.00      1.00        50\n",
      "           2       0.85      1.00      0.92        52\n",
      "           3       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.94       186\n",
      "   macro avg       0.95      0.93      0.93       186\n",
      "weighted avg       0.95      0.94      0.94       186\n",
      "\n",
      "{'SVM__C': 1, 'SVM__gamma': 0.01, 'knn__n_neighbors': 5, 'rf__n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using gridSearch\n",
    "params = {'SVM__C':[1, 10, 100],\n",
    "          'SVM__gamma':[0.1, 0.01],\n",
    "          'knn__n_neighbors': [1,3,5],\n",
    "          'rf__n_estimators':[300, 400],\n",
    "          }\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "voting_clf = grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "Voting_Accuracy=voting_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(Voting_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(voting_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91lBO_ZUYZ4j"
   },
   "outputs": [],
   "source": [
    "# open a file, where you ant to store the data\n",
    "file = open('Heart_Disease_Prediction_using_ECG.pkl', 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(voting_clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emu__tW7qP9b"
   },
   "source": [
    "## SAVE AND USE THE ABOVE MODEL IN THE STREAMLIT APP : **https://colab.research.google.com/drive/139YVmcUBCiP52J2sX3QE_eiu2sukVgpn?usp=sharing**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Merging_Scaled_1D_&_Trying_Different_CLassification_ML_Models_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
